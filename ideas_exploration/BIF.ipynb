{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "547bad71-5f7b-402d-a786-adc4d79da6bb",
   "metadata": {},
   "source": [
    "<img src=\"./SGLD_algo.png\" width=\"800\" height=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919e9061-dcf6-4bef-9bd0-3da356f7c0a2",
   "metadata": {},
   "source": [
    "SGLD optimizer updates parameters like this:\n",
    "$$\\Delta w_t = \\frac{\\epsilon}{2}\\left(\\frac{\\beta n}{m} \\sum_{i=1}^m \\nabla \\log p\\left(y_{l_i} \\mid x_{l_i}, w_t\\right)+\\gamma\\left(w_0-w_t\\right) - \\lambda w_t\\right) + N(0, \\epsilon\\sigma^2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dd9775-0486-4841-8c42-059bd3397084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from devinterp.optim.sgld import SGLD\n",
    "from devinterp.slt.callback import SamplerCallback\n",
    "from typing import Callable, Dict, List, Literal, Optional, Type, Union\n",
    "from devinterp.utils import (\n",
    "    USE_TPU_BACKEND,\n",
    "    EvaluateFn,\n",
    "    EvalResults,\n",
    "    default_nbeta,\n",
    "    get_init_loss_multi_batch,\n",
    ")\n",
    "if USE_TPU_BACKEND:\n",
    "    from devinterp.backends.tpu.slt.sampler import sample\n",
    "else:\n",
    "    from devinterp.backends.default.slt.sampler import sample\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "\n",
    "DEVICE = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743d56b7-f07b-49da-8cca-01a3057d01b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIFEstimator(SamplerCallback):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_chains: int,\n",
    "        num_draws: int,\n",
    "\n",
    "        num_data: int,\n",
    "        num_obs: int,\n",
    "        \n",
    "        init_loss: torch.Tensor,\n",
    "        device: Union[torch.device, str] = \"cpu\",\n",
    "        eval_field: List[str] = [\"loss\", \"obs\"],\n",
    "        nbeta: float = None,\n",
    "        temperature: float = None,\n",
    "    ):\n",
    "        self.num_chains = num_chains\n",
    "        self.num_draws = num_draws\n",
    "        self.num_data = num_data\n",
    "        self.num_obs = num_obs\n",
    "        \n",
    "        self.losses = torch.zeros((num_data, num_chains*num_draws), dtype=torch.float32).to(\n",
    "            device\n",
    "        )\n",
    "        self.observables = torch.zeros((num_obs, num_chains*num_draws), dtype=torch.float32).to(\n",
    "            device\n",
    "        )\n",
    "        self.init_loss = init_loss\n",
    "\n",
    "        assert nbeta is not None or temperature is not None, (\n",
    "            \"Please provide a value for nbeta.\"\n",
    "        )\n",
    "        if nbeta is None and temperature is not None:\n",
    "            nbeta = temperature\n",
    "            warnings.warn(\"Temperature is deprecated. Please use nbeta instead.\")\n",
    "            \n",
    "        self.nbeta = torch.tensor(nbeta, dtype=torch.float32).to(device)\n",
    "        self.temperature = temperature\n",
    "\n",
    "        self.device = device\n",
    "        self.eval_field = eval_field\n",
    "\n",
    "    def update(self, chain: int, draw: int, loss_vec: torch.tensor, obs_vec: torch.tensor):\n",
    "        if torch.isnan(loss_vec).any():\n",
    "            raise RuntimeError(f\"NaN detected in loss at chain {chain}, draw {draw}\")\n",
    "\n",
    "        col = chain * self.num_draws + draw\n",
    "        self.losses[:, col] = loss_vec.to(self.device)\n",
    "        self.observables[:, col] = obs_vec.to(self.device)\n",
    "\n",
    "\n",
    "    def get_results(self):\n",
    "        \"\"\"\n",
    "        :returns: A dict :python:`\n",
    "        {\"llc/mean\": llc_mean, \"llc/std\": llc_std, \"llc-chain/{i}\": llc_trace_per_chain, \"loss/trace\": loss_trace_per_chain}`. \n",
    "        (Only after running :python:`devinterp.slt.sampler.sample(..., [llc_estimator_instance], ...)`).\n",
    "        \"\"\"\n",
    "\n",
    "        init_loss = (\n",
    "            self.init_loss.item()\n",
    "            if isinstance(self.init_loss, torch.Tensor)\n",
    "            else self.init_loss\n",
    "        )\n",
    "\n",
    "\n",
    "        # calculating BIF matrix where BIF_ij = BIF(z_i, phi_j)\n",
    "        # returns matrix of (num_data x num_obs)\n",
    "        CT = self.num_chains * self.num_draws\n",
    "        multiplier_mat = (torch.eye(CT) - torch.full((CT,CT), 1/CT)).to(self.device)\n",
    "        multiplier_mat = torch.matmul(multiplier_mat, multiplier_mat.T)\n",
    "        BIF = 1/(CT - 1) * torch.matmul(torch.matmul(self.losses, multiplier_mat), self.observables.T)\n",
    "\n",
    "        return {\n",
    "            \"init_loss\": init_loss,\n",
    "            \"BIF\": BIF,\n",
    "            \"loss/trace\": self.losses.cpu().numpy(),\n",
    "            \"obs/trace\": self.observables.cpu().numpy(),\n",
    "        }\n",
    "\n",
    "    def finalize(self):\n",
    "        if os.environ.get(\"USE_SPMD\", \"0\") == \"1\" and not str(self.device).startswith(\n",
    "            \"cpu:\"\n",
    "        ):\n",
    "            if str(self.device).startswith(\"cuda\") and torch.cuda.device_count() > 1:\n",
    "                if torch.distributed.is_initialized():\n",
    "                    torch.distributed.barrier()\n",
    "                    torch.distributed.all_reduce(\n",
    "                        self.losses, op=torch.distributed.ReduceOp.AVG\n",
    "                    )\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        elif USE_TPU_BACKEND and str(self.device).startswith(\"xla:\"):\n",
    "            import torch_xla.core.xla_model as xm\n",
    "\n",
    "            if TPU_TYPE == \"v4\":\n",
    "                self.losses = xm.all_reduce(xm.REDUCE_SUM, self.losses)\n",
    "            elif TPU_TYPE == \"v2/v3\":\n",
    "                self.losses = self.losses.cpu()\n",
    "                if torch.distributed.is_initialized():\n",
    "                    torch.distributed.all_reduce(self.losses)\n",
    "                else:\n",
    "                    warnings.warn(\n",
    "                        \"torch.distributed has not been initialized. If running on TPU v2/v3, and you want to run chains in parallel, you need to initialize torch.distributed after calling xmp.spawn() as follows:\"\n",
    "                        \">>> import torch_xla.runtime as xr\"\n",
    "                        \">>> store = torch.distributed.TCPStore('127.0.0.1', 12345, 4, xr.global_ordinal() == 0)\"\n",
    "                        \">>> torch.distributed.init_process_group(backend='gloo', store=store, rank=xr.global_ordinal()//2, world_size=xr.world_size()//2)\"\n",
    "                    )\n",
    "\n",
    "            else:\n",
    "                raise NotImplementedError(f\"TPU type {TPU_TYPE} not supported\")\n",
    "        elif str(\n",
    "            self.device\n",
    "        ).startswith(\n",
    "            \"cuda\"\n",
    "        ):  # if we've ran on multi-GPU, we should do a reduce as well. see above for how this would work\n",
    "            try:\n",
    "                torch.distributed.all_reduce(self.losses)\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "    def __call__(self, chain: int, draw: int, **kwargs):\n",
    "        # eval_field = [\"loss\", \"obs\"]\n",
    "        print(kwargs[\"loss\"])\n",
    "        self.update(chain, draw, kwargs[\"loss\"], kwargs[\"results\"][\"obs\"])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06b9b1d-39c2-402e-821f-e5cfaa7d75d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_bif(\n",
    "    model: torch.nn.Module,\n",
    "    loader: DataLoader,\n",
    "    num_obs: int,\n",
    "    callbacks: List[Callable] = [],\n",
    "    evaluate: Optional[EvaluateFn] = None,\n",
    "    sampling_method: Type[torch.optim.Optimizer] = SGLD,\n",
    "    sampling_method_kwargs: Optional[\n",
    "        Dict[str, Union[float, Literal[\"adaptive\"]]]\n",
    "    ] = None,\n",
    "    num_draws: int = 100,\n",
    "    num_chains: int = 10,\n",
    "    num_burnin_steps: int = 0,\n",
    "    num_steps_bw_draws: int = 1,\n",
    "    init_loss: float = None,\n",
    "    gradient_accumulation_steps: int = 1,\n",
    "    cores: Union[int, List[Union[str, torch.device]]] = 1,\n",
    "    seed: Optional[Union[int, List[int]]] = None,\n",
    "    device: Union[torch.device, str] = torch.device(\"cuda\"),\n",
    "    gpu_idxs: Optional[List[int]] = None,\n",
    "    verbose: bool = True,\n",
    "    optimize_over_per_model_param: Optional[Dict[str, torch.Tensor]] = None,\n",
    "    online: bool = False,\n",
    "    dtype: Optional[torch.dtype] = None,\n",
    ") -> dict:\n",
    "\n",
    "    # Using hyperparameters from kreer BIF paper, see \"vision\" in table in paper\n",
    "    sampling_method_kwargs = dict()\n",
    "    sampling_method_kwargs[\"nbeta\"] = 10\n",
    "    sampling_method_kwargs[\"localization\"] = 1000\n",
    "    sampling_method_kwargs[\"lr\"] = 1e-4\n",
    "\n",
    "    if not init_loss:\n",
    "        init_loss = get_init_loss_multi_batch(\n",
    "            loader, num_chains * gradient_accumulation_steps, model, evaluate, device\n",
    "        )\n",
    "    \n",
    "    bif_estimator = BIFEstimator(\n",
    "            num_chains,\n",
    "            num_draws,\n",
    "            nbeta=sampling_method_kwargs[\"nbeta\"],\n",
    "            device=device,\n",
    "            init_loss=init_loss,\n",
    "            num_obs=num_obs, \n",
    "            num_data = len(loader)\n",
    "        )\n",
    "    \n",
    "    callbacks = [bif_estimator, *callbacks]\n",
    "\n",
    "    sample(\n",
    "        model=model,\n",
    "        loader=loader,\n",
    "        evaluate=evaluate,\n",
    "        sampling_method=sampling_method,\n",
    "        sampling_method_kwargs=sampling_method_kwargs,\n",
    "        num_draws=num_draws,\n",
    "        num_chains=num_chains,\n",
    "        num_burnin_steps=num_burnin_steps,\n",
    "        num_steps_bw_draws=num_steps_bw_draws,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        cores=cores,\n",
    "        seed=seed,\n",
    "        device=device,\n",
    "        verbose=verbose,\n",
    "        callbacks=callbacks,\n",
    "        optimize_over_per_model_param=optimize_over_per_model_param,\n",
    "        gpu_idxs=gpu_idxs,\n",
    "        dtype=dtype,\n",
    "        init_loss=init_loss,\n",
    "    )\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for callback in callbacks:\n",
    "        if hasattr(callback, \"get_results\"):\n",
    "            results.update(callback.get_results())\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101684d4-e02b-493f-bee4-0b1ed62e988f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619a9dd0-37a5-4fd4-9084-887a40ff5708",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, non_linear_activation=nn.ReLU(), input_dim=2, output_dim=2):\n",
    "        super().__init__()\n",
    "        self.non_linear_activation = non_linear_activation\n",
    "\n",
    "        self.f = nn.Sequential(\n",
    "            nn.Linear(input_dim, 10),\n",
    "            self.non_linear_activation,\n",
    "            nn.Linear(10, output_dim),\n",
    "            nn.LogSoftmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e318e2f-5ae6-4f21-addd-be9cdf3b8d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_synthetic_data(N=50):\n",
    "    xs=ys=-2\n",
    "    xe=ye=2\n",
    "    \n",
    "    X1 = np.linspace(xs,xe,N)\n",
    "    X2 = np.linspace(ys,ye,N)\n",
    "    \n",
    "    X = np.meshgrid(X1,X2)\n",
    "    X = np.stack([x.ravel()[:None] for x in X], axis=-1)\n",
    "    \n",
    "    Y = ((np.square(X[:,0]) + np.square(X[:,1])) <= 1).astype(\"int\")\n",
    "\n",
    "    return torch.from_numpy(X).float(), torch.from_numpy(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ca92ac-4781-4de6-8d0b-6aae1a65987e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(N=40):\n",
    "    data,labels=gen_synthetic_data(N=N)\n",
    "    data = data.to(DEVICE)\n",
    "    labels = labels.to(DEVICE)\n",
    "\n",
    "    net = NeuralNet().to(DEVICE)\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    optim = torch.optim.Adam(net.parameters())\n",
    "\n",
    "    EPOCHS = 30\n",
    "    BATCH = 50\n",
    "\n",
    "    ls = []\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        eloss = 0\n",
    "        for b in range(100):\n",
    "            indexes = torch.randperm(data.shape[0])[:BATCH]\n",
    "            batch = data[indexes]\n",
    "            target = labels[indexes]\n",
    "            \n",
    "            out = net(batch)\n",
    "            loss = loss_fn(out, target)\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            eloss += loss.item()\n",
    "\n",
    "        tloss = eloss / data.shape[0]\n",
    "        ls.append(tloss)\n",
    "\n",
    "    return data,labels,net,ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90db8d1-d968-4580-a0d5-6a5d52d721dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data,labels,net,ls = train(N=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f73beb2-187a-47a3-ae1a-2b0402e964a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output=None\n",
    "with torch.no_grad():\n",
    "    output = torch.argmax(net(data), dim=-1).cpu().numpy()\n",
    "X = data.cpu().numpy()\n",
    "\n",
    "valid_data = ((torch.rand((500,2)) - 0.5) * 4).to(DEVICE)\n",
    "output_valid=None\n",
    "with torch.no_grad():\n",
    "    output_valid = torch.argmax(net(valid_data), dim=-1).cpu().numpy()\n",
    "\n",
    "fig,arr = plt.subplots(1,4,figsize=(21,5))\n",
    "\n",
    "for a in arr:\n",
    "    a.set_xticks([])\n",
    "    a.set_yticks([])\n",
    "\n",
    "arr[0].plot(np.arange(len(ls)),ls)\n",
    "arr[0].set_title(\"epoch vs. loss\")\n",
    "\n",
    "for i in range(2):\n",
    "    ind_i = labels.cpu() == i\n",
    "    arr[1].scatter(X[ind_i,0], X[ind_i,1], label=\"positive\" if i > 0 else \"negative\")\n",
    "    arr[1].set_title(\"training data\")\n",
    "\n",
    "fig.legend()\n",
    "\n",
    "for i in range(2):\n",
    "    ind_i = output == i\n",
    "    arr[2].scatter(X[ind_i,0], X[ind_i,1], label=i)\n",
    "    arr[2].set_title(\"output after training\")\n",
    "\n",
    "for i in range(2):\n",
    "    ind_i = output_valid == i\n",
    "    arr[3].scatter(valid_data[ind_i,0].cpu(), valid_data[ind_i,1].cpu(), label=i)\n",
    "    arr[3].set_title(\"validation data\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631acbfd-c782-404c-8c90-b45d21b8a43e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd1bddd-1586-4044-8703-416eee76aaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_function(model: nn.Module, data: torch.Tensor) -> EvalResults:\n",
    "    inputs, outputs = data\n",
    "\n",
    "    def get_test_pt(point):\n",
    "        x1,x2=point\n",
    "        norm = np.square(x1) + np.square(x2)\n",
    "        lab = 1 if norm <= 1 else 0\n",
    "        return (torch.tensor([x1,x2]).float(), torch.tensor(lab))\n",
    "        \n",
    "    x,y = get_test_pt((0,0))\n",
    "\n",
    "    return {\n",
    "        \"loss\": torch.nn.functional.nll_loss(model(inputs), outputs),\n",
    "        \"obs\": torch.tensor(torch.nn.functional.nll_loss(model(x), y).item())\n",
    "    }\n",
    "\n",
    "loader = torch.utils.data.DataLoader(list(zip(data,labels)), shuffle=False, batch_size=1)\n",
    "\n",
    "test = estimate_bif(\n",
    "    model=net.to(DEVICE),\n",
    "    loader = loader,\n",
    "    evaluate = evaluate_function,\n",
    "    num_obs=1,\n",
    "    device=\"cpu\",\n",
    "    num_chains=1,\n",
    "    num_draws=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014888ea-2db6-4973-b3c2-90c50f02d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIF = test[\"BIF\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720ec23c-7556-4970-99da-eda950e4ffa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test[\"obs/trace\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3c0e55-2b9f-4c87-be46-c3d3abffdf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test[\"loss/trace\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f6cba3-3bae-417f-af22-a0cda1553219",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.cpu().numpy()\n",
    "fig, arr = plt.subplots(1,1,figsize=(6,5))\n",
    "scatter = arr.scatter(X[:,0], X[:,1], c=BIF[:,0])\n",
    "arr.scatter(TESTX[0], TESTX[1], c=\"red\")\n",
    "fig.colorbar(scatter)\n",
    "plt.title(\"influence of training data on point (red)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3aee8c-59e4-466f-9738-572adaa0fe1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
