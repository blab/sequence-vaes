{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "547bad71-5f7b-402d-a786-adc4d79da6bb",
   "metadata": {},
   "source": [
    "<img src=\"./SGLD_algo.png\" width=\"800\" height=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919e9061-dcf6-4bef-9bd0-3da356f7c0a2",
   "metadata": {},
   "source": [
    "SGLD optimizer updates parameters like this:\n",
    "$$\\Delta w_t = \\frac{\\epsilon}{2}\\left(\\frac{\\beta n}{m} \\sum_{i=1}^m \\nabla \\log p\\left(y_{l_i} \\mid x_{l_i}, w_t\\right)+\\gamma\\left(w_0-w_t\\right) - \\lambda w_t\\right) + N(0, \\epsilon\\sigma^2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ac1ba0d-e3f3-4168-a8da-863c6bae9c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0],\n",
      "        [4, 1]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1,0],[2,1]])\n",
    "b = torch.tensor([[0,1],[1,0]])\n",
    "\n",
    "print(torch.matmul(a,a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2dd9775-0486-4841-8c42-059bd3397084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from devinterp.optim.sgld import SGLD\n",
    "from devinterp.slt.callback import SamplerCallback\n",
    "from typing import Callable, List, Union\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "743d56b7-f07b-49da-8cca-01a3057d01b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIFEstimator(SamplerCallback):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_chains: int,\n",
    "        num_draws: int,\n",
    "\n",
    "        num_data: int,\n",
    "        num_obs: int,\n",
    "        \n",
    "        init_loss: torch.Tensor,\n",
    "        device: Union[torch.device, str] = \"cpu\",\n",
    "        eval_field: List[str] = [\"loss\", \"obs\"]\n",
    "        nbeta: float = None,\n",
    "        temperature: float = None,\n",
    "    ):\n",
    "        self.num_chains = num_chains\n",
    "        self.num_draws = num_draws\n",
    "        self.num_data = num_data\n",
    "        self.num_obs = num_obs\n",
    "        \n",
    "        self.losses = torch.zeros((num_data, num_chains*num_draws), dtype=torch.float32).to(\n",
    "            device\n",
    "        )\n",
    "        self.observables = torch.zeros((num_obs, num_chains*num_draws), dtype=torch.float32).to(\n",
    "            device\n",
    "        )\n",
    "        self.init_loss = init_loss\n",
    "\n",
    "        assert nbeta is not None or temperature is not None, (\n",
    "            \"Please provide a value for nbeta.\"\n",
    "        )\n",
    "        if nbeta is None and temperature is not None:\n",
    "            nbeta = temperature\n",
    "            warnings.warn(\"Temperature is deprecated. Please use nbeta instead.\")\n",
    "            \n",
    "        self.nbeta = torch.tensor(nbeta, dtype=torch.float32).to(device)\n",
    "        self.temperature = temperature\n",
    "\n",
    "        self.device = device\n",
    "        self.eval_field = eval_field\n",
    "\n",
    "    def update(self, chain: int, draw: int, loss_vec: torch.tensor, obs_vec: torch.tensor):\n",
    "        if torch.isnan(loss).any():\n",
    "            raise RuntimeError(f\"NaN detected in loss at chain {chain}, draw {draw}\")\n",
    "\n",
    "        col = (chain - 1) * self.num_chains + draw\n",
    "        self.losses[:, col] = loss_vec.to(self.device)\n",
    "        self.observables[:, col] = obs_vec.to(self.device)\n",
    "\n",
    "\n",
    "    def get_results(self):\n",
    "        \"\"\"\n",
    "        :returns: A dict :python:`\n",
    "        {\"llc/mean\": llc_mean, \"llc/std\": llc_std, \"llc-chain/{i}\": llc_trace_per_chain, \"loss/trace\": loss_trace_per_chain}`. \n",
    "        (Only after running :python:`devinterp.slt.sampler.sample(..., [llc_estimator_instance], ...)`).\n",
    "        \"\"\"\n",
    "\n",
    "        init_loss = (\n",
    "            self.init_loss.item()\n",
    "            if isinstance(self.init_loss, torch.Tensor)\n",
    "            else self.init_loss\n",
    "        )\n",
    "\n",
    "\n",
    "        # calculating BIF matrix where BIF_ij = BIF(z_i, phi_j)\n",
    "        # returns matrix of (num_data x num_obs)\n",
    "        CT = self.num_chains * self.num_draws\n",
    "        multiplier_mat = (torch.eye(CT) - torch.full((CT,CT), 1/CT)).to(self.device)\n",
    "        multiplier_mat = torch.matmul(multiplier_mat, multiplier_mat.T)\n",
    "        BIF = 1/(CT - 1) * torch.matmul(torch.matmul(self.losses, multiplier_mat), self.observables.T)\n",
    "\n",
    "        return {\n",
    "            \"init_loss\": init_loss,\n",
    "            \"BIF\": BIF\n",
    "            **{\n",
    "                f\"llc-chain/{i}\": self.llc_per_chain[i].cpu().numpy().item()\n",
    "                for i in range(self.num_chains)\n",
    "            },\n",
    "            \"loss/trace\": self.losses.cpu().numpy(),\n",
    "            \"obs/trace\": self.observables.cpu().numpy(),\n",
    "        }\n",
    "\n",
    "    def __call__(self, chain: int, draw: int, **kwargs):\n",
    "        # eval_field = [\"loss\", \"obs\"]\n",
    "        self.update(chain, draw, kwargs[self.eval_field[0]], kwargs[self.eval_field[1]])\n",
    "\n",
    "    def finalize(self):\n",
    "        if os.environ.get(\"USE_SPMD\", \"0\") == \"1\" and not str(self.device).startswith(\n",
    "            \"cpu:\"\n",
    "        ):\n",
    "            if str(self.device).startswith(\"cuda\") and torch.cuda.device_count() > 1:\n",
    "                if torch.distributed.is_initialized():\n",
    "                    torch.distributed.barrier()\n",
    "                    torch.distributed.all_reduce(\n",
    "                        self.losses, op=torch.distributed.ReduceOp.AVG\n",
    "                    )\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        elif USE_TPU_BACKEND and str(self.device).startswith(\"xla:\"):\n",
    "            import torch_xla.core.xla_model as xm\n",
    "\n",
    "            if TPU_TYPE == \"v4\":\n",
    "                self.losses = xm.all_reduce(xm.REDUCE_SUM, self.losses)\n",
    "            elif TPU_TYPE == \"v2/v3\":\n",
    "                self.losses = self.losses.cpu()\n",
    "                if torch.distributed.is_initialized():\n",
    "                    torch.distributed.all_reduce(self.losses)\n",
    "                else:\n",
    "                    warnings.warn(\n",
    "                        \"torch.distributed has not been initialized. If running on TPU v2/v3, and you want to run chains in parallel, you need to initialize torch.distributed after calling xmp.spawn() as follows:\"\n",
    "                        \">>> import torch_xla.runtime as xr\"\n",
    "                        \">>> store = torch.distributed.TCPStore('127.0.0.1', 12345, 4, xr.global_ordinal() == 0)\"\n",
    "                        \">>> torch.distributed.init_process_group(backend='gloo', store=store, rank=xr.global_ordinal()//2, world_size=xr.world_size()//2)\"\n",
    "                    )\n",
    "\n",
    "            else:\n",
    "                raise NotImplementedError(f\"TPU type {TPU_TYPE} not supported\")\n",
    "        elif str(\n",
    "            self.device\n",
    "        ).startswith(\n",
    "            \"cuda\"\n",
    "        ):  # if we've ran on multi-GPU, we should do a reduce as well. see above for how this would work\n",
    "            try:\n",
    "                torch.distributed.all_reduce(self.losses)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        avg_losses = self.losses.mean(axis=1)\n",
    "        # bypass automatic bfloat16 issues\n",
    "        if os.environ.get(\"XLA_USE_BF16\", \"0\") == \"1\" and str(self.device).startswith(\n",
    "            \"xla:\"\n",
    "        ):\n",
    "            self.llc_per_chain = self.nbeta.to(device=\"cpu\", dtype=torch.float32) * (\n",
    "                avg_losses.to(device=\"cpu\", dtype=torch.float32)\n",
    "                - self.init_loss.to(device=\"cpu\", dtype=torch.float32)\n",
    "            )\n",
    "        elif (\n",
    "            str(self.device).startswith(\"cuda\")\n",
    "            and os.environ.get(\"USE_SPMD\", \"0\") == \"1\"\n",
    "        ):\n",
    "            self.llc_per_chain = self.nbeta.to(device=\"cpu\", dtype=torch.float32) * (\n",
    "                avg_losses.to(device=\"cpu\", dtype=torch.float32)\n",
    "                - self.init_loss.to(device=\"cpu\", dtype=torch.float32)\n",
    "            )\n",
    "        else:\n",
    "            self.llc_per_chain = self.nbeta * (avg_losses - self.init_loss)\n",
    "        \n",
    "        self.llc_mean = self.llc_per_chain.mean()\n",
    "        self.llc_std = self.llc_per_chain.std()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6f4a360-1b6e-470f-8bdf-9d7a442e7032",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53e8a1ac-19ee-4047-a450-1f7a882579b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "a = {\"one\":1}\n",
    "print(a.setdefault(\"two\",6))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
