{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c0b583e-c368-4d34-8a03-700726373987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, PreTrainedTokenizerFast, DataCollatorForLanguageModeling\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append(\"../VAE_standard\")\n",
    "from models import DNADataset, ALPHABET, SEQ_LENGTH, LATENT_DIM, VAE\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "import utils\n",
    "\n",
    "import Bio.Data.CodonTable\n",
    "\n",
    "from devinterp.utils import (\n",
    "    EvaluateFn,\n",
    "    EvalResults,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1423ee03-9648-4cb3-9a69-5d56b615a281",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "333576cd-2c16-4ad7-ad6c-58ca18d565e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKEN_LENGTH = 1020\n",
    "num_masks = 3\n",
    "\n",
    "TEST_SEQ = 1\n",
    "TRAIN_CUTOFF = 1000\n",
    "TEST_TOKEN = 0\n",
    "\n",
    "DEVICE = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7b17c86-a00d-4a24-b9de-1ec95e57a97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done extracting sequences!\n",
      "done removing gaps!\n",
      "done creating element copies!\n"
     ]
    }
   ],
   "source": [
    "dataset = DNADataset(f\"../data/training_spike.fasta\")\n",
    "sequences = [utils.get_genome(np.dot(x[0], np.arange(len(ALPHABET)))) for x in dataset]\n",
    "print(\"done extracting sequences!\")\n",
    "\n",
    "str_sequences = [[x for x in seq][:MAX_TOKEN_LENGTH] for seq in np.unique([\"\".join(x).replace(\"-\",\"\") for x in sequences])]\n",
    "print(\"done removing gaps!\")\n",
    "\n",
    "test_seq = str_sequences[TEST_SEQ-1:TEST_SEQ]\n",
    "masked_seqs = [seq.copy() for j in range(MAX_TOKEN_LENGTH // num_masks) for seq in test_seq]\n",
    "labeled_seqs = [seq for j in range(MAX_TOKEN_LENGTH // num_masks) for seq in test_seq]\n",
    "print(\"done creating element copies!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0506150f-4642-404e-9afb-b18d3d035f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(masked_seqs)):\n",
    "    j = i % (MAX_TOKEN_LENGTH // num_masks)\n",
    "    for k in range(num_masks * j, num_masks * (j+1)):\n",
    "        masked_seqs[i][k] = \"<mask>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7ca739e-90d8-4bfd-8875-69cb61970360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done creating bif masks!\n",
      "done creating bif labels!\n",
      "done creating sgld dataset!\n"
     ]
    }
   ],
   "source": [
    "masked_seqs = [\"\".join(x) for x in masked_seqs]\n",
    "print(\"done creating bif masks!\")\n",
    "\n",
    "labeled_seqs = [\"\".join(seq) for seq in labeled_seqs]\n",
    "print(\"done creating bif labels!\")\n",
    "\n",
    "train_data = [\"\".join(seq).replace(\"-\",\"\")[:MAX_TOKEN_LENGTH] for seq in sequences[:TRAIN_CUTOFF]]\n",
    "print(\"done creating sgld dataset!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "777b69e5-b024-4d6a-bf45-b3fe3a0a260b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340\n",
      "340\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(masked_seqs))\n",
    "print(len(masked_seqs))\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f8e5910-5dd9-4910-94b1-e505d64461dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = Bio.Data.CodonTable.standard_dna_table\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ac134d-0cd7-424e-bdbf-62b663df1616",
   "metadata": {},
   "source": [
    "## Loading ESM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98b524f0-d3ca-49f3-b462-de335ecec221",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/averma2/miniforge3/envs/Moreta_env/lib/python3.9/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "tokenizer: \n",
    "input_ids - torch.LongTensor of shape (batch_size, sequence_length)\n",
    "attention_mask - torch.Tensor of shape (batch_size, sequence_length), Mask values selected in {0,1}, where 0 := masked, 1 := not masked\n",
    "\"\"\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"facebook/esm2_t6_8M_UR50D\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08a1554d-e14d-40aa-9ef8-a9530ab2f24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15, return_tensors=\"pt\")\n",
    "sgld_dataset = tokenizer(text=train_data, return_tensors=\"pt\", add_special_tokens=False, truncation=False)[\"input_ids\"]\n",
    "sgld_inputs, sgld_labels = data_collator.torch_mask_tokens(sgld_dataset)\n",
    "\n",
    "sgld_inputs = sgld_inputs.to(DEVICE)\n",
    "sgld_labels = sgld_labels.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84ddba96-3c4d-4f38-8a7a-1031863531ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([340, 1020])\n",
      "torch.Size([340, 1020])\n"
     ]
    }
   ],
   "source": [
    "bif_inputs = tokenizer(text=masked_seqs, return_tensors=\"pt\", add_special_tokens=False, truncation=False)\n",
    "bif_labels = tokenizer(text=labeled_seqs, return_tensors=\"pt\", add_special_tokens=False, truncation=False)[\"input_ids\"]\n",
    "bif_labels = torch.where(bif_inputs[\"input_ids\"] == tokenizer.mask_token_id, bif_labels, -100)\n",
    "\n",
    "bif_labels = bif_labels.to(DEVICE)\n",
    "bif_inputs[\"input_ids\"] = bif_inputs[\"input_ids\"].to(DEVICE)\n",
    "bif_inputs[\"attention_mask\"] = bif_inputs[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "print(bif_inputs[\"input_ids\"].shape)\n",
    "print(bif_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76af1c0b-feec-4aa6-b16c-5a658666a925",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=40\n",
    "bif_dataloader = torch.utils.data.DataLoader(list(zip(zip(bif_inputs[\"input_ids\"], bif_inputs[\"attention_mask\"]), bif_labels)), batch_size=BATCH_SIZE, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7da85f37-6279-4498-9e61-8ea4cc7cb856",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq = tokenizer(text=masked_seqs[TEST_TOKEN], return_tensors=\"pt\", add_special_tokens=False, truncation=False)\n",
    "test_seq_label = tokenizer(text=labeled_seqs[TEST_TOKEN], return_tensors=\"pt\", add_special_tokens=False, truncation=False)[\"input_ids\"]\n",
    "\n",
    "test_seq_label = test_seq_label.squeeze().to(DEVICE)\n",
    "test_seq[\"input_ids\"] = test_seq[\"input_ids\"].to(DEVICE)\n",
    "test_seq[\"attention_mask\"] = test_seq[\"attention_mask\"].to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22887e2f-228f-4453-a21b-e0d0c7f0f67d",
   "metadata": {},
   "source": [
    "## Per-token BIF algorithm\n",
    "<img src=\"./per_tok_BIF.png\" width=\"1000\" height=\"1000\"/>\n",
    "\n",
    "### For MLM models like ESM, $\\ell_i(w) = -\\sum_{s\\in [M]}^S log(p(z_{i,s} | z_{i,[M]^c}))$, where $[M]$ is random mask. Then, I define $\\ell_{i,s}(w) = -log(p(z_{i,s} | z_{i,\\{s\\}^c}))$, i.e. the loss of a single token is the cross-entropy loss of masking that single token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f9a306-527f-4810-bf19-c684f6c64ce1",
   "metadata": {},
   "source": [
    "### Define sequences $\\{x_i\\}_{i=1}^N$ where $x_i = (x_{i,1},...x_{i,S})$ and $S:=$length of sequence. Then, I fix one amino-acid $x_{i,j}$ and calculate $BIF(*,x_{i,j}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c9989aa-6508-4feb-bcf7-8c03ca4027ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_vec(\n",
    "    dataloader=bif_dataloader, \n",
    "    vocab_size=tokenizer.vocab_size, \n",
    "    model=model, num_data = MAX_TOKEN_LENGTH // num_masks,\n",
    "    batch_size = BATCH_SIZE\n",
    "):\n",
    "    \"\"\"\n",
    "    dataloader: (torch.utils.DataLoader) where each element is of the form ((input, attention_mask), label)\n",
    "    vocab_size: number of unique tokens in tokenizer (tokenizer.vocab_size)\n",
    "    model: ESM model\n",
    "\n",
    "    returns: array of losses which is used to calculate BIF matrix\n",
    "    \"\"\"\n",
    "    loss_vec = torch.zeros(num_data).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        for i, ((test_input, test_mask), test_labels) in enumerate(dataloader):\n",
    "            bsize = test_input.shape[0]\n",
    "            outputs = model(test_input, attention_mask=test_mask)[\"logits\"] # returns tensor of size (batch, seq_length, vocab_size)\n",
    "            batch_loss = torch.sum(nn.functional.cross_entropy(outputs.view(-1,vocab_size), target=test_labels.view(-1), ignore_index=-100, reduction=\"none\").view(bsize, -1), dim=-1)\n",
    "            if i < len(dataloader) - 1:\n",
    "                loss_vec[i * batch_size : (i+1) * batch_size] = batch_loss\n",
    "            else:\n",
    "                loss_vec[i * batch_size:] = batch_loss\n",
    "            \n",
    "    return loss_vec\n",
    "\n",
    "def get_obs(\n",
    "    test_seq=((test_seq[\"input_ids\"], test_seq[\"attention_mask\"]), test_seq_label), \n",
    "    model=model\n",
    "):\n",
    "    \"\"\"\n",
    "    test_seq: tuple of form ((input, attention_mask), label) -- attention_mask should be tensor of same length as input tensor and containing only 1s\n",
    "        input has shape (1, seq_length)\n",
    "        attention_mask has shape (1, seq_length)\n",
    "        label has shape (seq_length)\n",
    "    model: ESM model\n",
    "\n",
    "    returns: loss value used to calculate BIF_matrix\n",
    "    \"\"\"\n",
    "    (test_input, test_mask), test_label = test_seq\n",
    "    with torch.no_grad():\n",
    "        test_output = model(test_input, attention_mask=test_mask)[\"logits\"].squeeze()\n",
    "        return nn.functional.cross_entropy(test_output, target=test_label, ignore_index=-100, reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62b72ae4-17a7-47e7-9004-854a527a047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_function(model: nn.Module, data: torch.Tensor, device=DEVICE) -> EvalResults:\n",
    "    \"\"\"\n",
    "    model (torch.nn.module): ESM model\n",
    "    data (torch.Tensor): minibatch data used to update weights of model in SGLD\n",
    "    device (String): either \"cuda\" or \"cpu\"\n",
    "\n",
    "    returns:\n",
    "    dictionary with \n",
    "    (i) loss - loss used to update weights of model\n",
    "    (ii) obs - calculated observables (in this case cross-entropy loss on one test token) as described above\n",
    "    (iii) loss_vec - the losses evaluated at every training point for the model used to compute the BIF as described in the algorithm at the top\n",
    "    \"\"\"\n",
    "\n",
    "    inputs, targets = data\n",
    "    loss = model(inputs, targets).loss\n",
    "\n",
    "    return {\n",
    "        \"loss\": torch.nn.functional.cross_entropy(model(inputs), outputs),\n",
    "        \"obs\": get_obs(),\n",
    "        \"loss_vec\": get_loss_vec()\n",
    "    }\n",
    "\n",
    "sgld_dataloader = torch.utils.data.DataLoader(zip(sgld_inputs, sgld_labels), batch_size=BATCH_SIZE, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f440d1-136c-4262-8069-1cbc51966ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
