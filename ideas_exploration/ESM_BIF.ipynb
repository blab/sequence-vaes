{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0b583e-c368-4d34-8a03-700726373987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append(\"../VAE_standard\")\n",
    "from models import DNADataset, ALPHABET, SEQ_LENGTH, LATENT_DIM, VAE\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "import utils\n",
    "\n",
    "import Bio.Data.CodonTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1086529b-7849-4fd5-ae1b-f4f52d19b31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "tokenizer: \n",
    "input_ids - torch.LongTensor of shape (batch_size, sequence_length)\n",
    "attention_mask - torch.Tensor of shape (batch_size, sequence_length), Mask values selected in {0,1}, where 0 := masked, 1 := not masked\n",
    "\"\"\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "\n",
    "MAX_TOKEN_LENGTH = 1024\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8e5910-5dd9-4910-94b1-e505d64461dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Bio.Data.CodonTable.standard_dna_table\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b17c86-a00d-4a24-b9de-1ec95e57a97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DNADataset(f\"../data/training_spike.fasta\")\n",
    "sequences = [utils.get_genome(np.dot(x[0], np.arange(len(ALPHABET)))) for x in dataset]\n",
    "str_sequences = [\"\".join(seq).replace(\"-\",\"\") for seq in sequences]\n",
    "codon_sequences = [[x.forward_table.get(seq[3*i:3*i+3], \"stop\") for i in range(len(seq) // 3 - 1)] for seq in str_sequences]\n",
    "codon_subsequences = [s[:MAX_TOKEN_LENGTH] for s in codon_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03485c38-e839-400c-b3bd-d2ea30bc7dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_codon_subsequences = [[seq[:N] + [\"<mask>\"] + seq[(N+1):] for N in range(MAX_TOKEN_LENGTH)] for seq in codon_subsequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca04ae6-45d4-48fc-8e7a-5d7a14e5895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = sequences[0]\n",
    "codon_seq = \n",
    "codon_subseq = codon_seq[:MAX_TOKEN_LENGTH]\n",
    "\n",
    "print(len(seq))\n",
    "print(len(codon_seq))\n",
    "print(len(codon_subseq))\n",
    "\n",
    "N = 0\n",
    "masked_codon_subseq = codon_subseq[:N] + [\"<mask>\"] + codon_subseq[(N+1):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde55b88-409f-4f93-aead-795f9cda2c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = tokenizer(\"\".join(masked_codon_subseq), return_tensors=\"pt\", add_special_tokens=False)\n",
    "test_label = tokenizer(\"\".join(codon_subseq), return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"]\n",
    "test_label = torch.where(test_input[\"input_ids\"] == tokenizer.mask_token_id, test_label, -100)\n",
    "\n",
    "print(test_input)\n",
    "print(test_label)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    test = model(**test_input, labels=test_label)\n",
    "    print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22887e2f-228f-4453-a21b-e0d0c7f0f67d",
   "metadata": {},
   "source": [
    "## Per-token BIF algorithm\n",
    "<img src=\"./per_tok_BIF.png\" width=\"1000\" height=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f9a306-527f-4810-bf19-c684f6c64ce1",
   "metadata": {},
   "source": [
    "### Define sequences $\\{x_i\\}_{i=1}^N$ where $x_i = (x_{i,1},...x_{i,S})$ and $S:=$length of sequence. Then, I fix one amino-acid $x_{i,j}$ and calculate $BIF(*,x_{i,j}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4599e13-7885-4167-9067-9fa3a6074965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_vec():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f440d1-136c-4262-8069-1cbc51966ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
