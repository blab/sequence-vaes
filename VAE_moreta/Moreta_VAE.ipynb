{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOObErWxrF1ljHzEDoSR9zc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tIAn7xYSmQ2k","executionInfo":{"status":"ok","timestamp":1752084448250,"user_tz":420,"elapsed":2070,"user":{"displayName":"Aayush Verma","userId":"08474324267284002295"}},"outputId":"49e6efdb-f33d-4f23-ebe1-be660a790f09"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!apt install qtbase5-dev qt5-qmake"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DaSmJTwlUeH1","executionInfo":{"status":"ok","timestamp":1752084451569,"user_tz":420,"elapsed":3318,"user":{"displayName":"Aayush Verma","userId":"08474324267284002295"}},"outputId":"ac004142-b2c9-4ea4-8fb8-3f3fc8189c13"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","qt5-qmake is already the newest version (5.15.3+dfsg-2ubuntu0.2).\n","qtbase5-dev is already the newest version (5.15.3+dfsg-2ubuntu0.2).\n","0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"]}]},{"cell_type":"code","source":["!pip install biopython; pip install gdown; pip3 install opencv-python; pip install PyQt5; pip install ete3;"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EmVygT7Ayygh","executionInfo":{"status":"ok","timestamp":1752084469301,"user_tz":420,"elapsed":17730,"user":{"displayName":"Aayush Verma","userId":"08474324267284002295"}},"outputId":"ecd5f56c-b679-48bc-ddff-e9277d989014"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: biopython in /usr/local/lib/python3.11/dist-packages (1.85)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from biopython) (2.0.2)\n","Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\n","Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.7)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.14.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.6.15)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n","Requirement already satisfied: PyQt5 in /usr/local/lib/python3.11/dist-packages (5.15.11)\n","Requirement already satisfied: PyQt5-sip<13,>=12.15 in /usr/local/lib/python3.11/dist-packages (from PyQt5) (12.17.0)\n","Requirement already satisfied: PyQt5-Qt5<5.16.0,>=5.15.2 in /usr/local/lib/python3.11/dist-packages (from PyQt5) (5.15.17)\n","Requirement already satisfied: ete3 in /usr/local/lib/python3.11/dist-packages (3.1.3)\n"]}]},{"cell_type":"code","source":["import warnings\n","from collections import namedtuple\n","import pprint\n","import os\n","import numpy as np\n","from Bio import SeqIO\n","import gdown"],"metadata":{"id":"WpcbwehryBOu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from scipy.sparse import coo_matrix\n","import argparse\n","import dill\n","import ast\n","import matplotlib.pyplot as plt\n","from matplotlib.pyplot import cm\n","from matplotlib.colors import LogNorm\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import statistics\n","import seaborn as sns\n","from collections import defaultdict,namedtuple\n","import pickle\n","#Biopython\n","import Bio.PDB as PDB\n","from Bio.PDB.Polypeptide import PPBuilder, CaPPBuilder\n","from Bio.Data.IUPACData import protein_letters_3to1\n","from Bio.SeqRecord import SeqRecord\n","from Bio.Seq import Seq\n","from Bio import BiopythonWarning\n","from Bio import AlignIO, SeqIO\n","from Bio.PDB.PDBList import PDBList\n","import Bio.Align\n","from Bio.Align.Applications import MafftCommandline\n","from Bio.Phylo.TreeConstruction import *\n","from Bio import Phylo\n","#Numpy\n","import numpy as np\n","import numpy.random as npr\n","import pandas as pd\n","import matplotlib\n","import re\n","\n","\n","from ete3 import Tree as TreeEte3\n","from ete3 import TreeStyle,NodeStyle, AttrFace"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QOhwzwUbTtQ0","executionInfo":{"status":"ok","timestamp":1752084473085,"user_tz":420,"elapsed":3778,"user":{"displayName":"Aayush Verma","userId":"08474324267284002295"}},"outputId":"43b6db39-c2dd-4556-96e9-28c2e8c741a5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/Bio/Application/__init__.py:39: BiopythonDeprecationWarning: The Bio.Application modules and modules relying on it have been deprecated.\n","\n","Due to the on going maintenance burden of keeping command line application\n","wrappers up to date, we have decided to deprecate and eventually remove these\n","modules.\n","\n","We instead now recommend building your command line and invoking it directly\n","with the subprocess module.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["\n","def create_dataset(name_file,\n","                   #args,\n","                   one_hot_encoding,\n","                   min_len=30,\n","                   fasta_file=None,\n","                   PDB_folder=None,\n","                   alignment_file=None,\n","                   tree_file = None,\n","                   pfam_dict= None,\n","                   method=\"iqtree\",\n","                   aa_probs=21,\n","                   rename_internal_nodes=False,\n","                   storage_folder=\"/content/drive/MyDrive/bedford_lab/code/DRAUPNIR_ASR/data\"):\n","    \"\"\" Combination function to create the dataset and additional files (i.e dictionaries) that Draupnir uses for inference\n","    in:\n","        :param str name_file : dataset name\n","        :param bool one_hot_encoding: {True,False} WARNING: One hot encoding is faulty, needs to be fixed, DO NOT USE\n","        :param int min_len: minimum length of the sequence, drops out sequences smaller than this\n","        :param str fasta_file: path to fasta with unaligned sequences\n","        :param str or None PDB_folder: Folder with PDB files from where to extract sequences and angles\n","        :param str or None alignment_file: path to fasta with aligned sequences\n","        :param str or None tree_file: path to newick tree, format 1 (ete3 nomenclature)\n","        :param dict or None pfam_dict: dictionary with PDB files names\n","        :param str method: tree inference methodology,\n","                          \"iqtree\": for ML tree inference by IQtree (make sure is installed globally),\n","                          \"nj\": for neighbour joining unrooted tree inference (biopython),\n","                          \"nj_rooted\": for NJ rooted tree inference (selects a root based on the distances beetwen nodes) (biopython),\n","                          \"upgma\": UPGMA (biopython),\n","                          \"rapidnj\": inference of Fast NJ unrooted inference (make sure is installed globally),\n","        aa_probs: amino acid probabilities\n","        rename_internal_nodes: {True,False} use different names for the internal/ancestral nodes from the ones given in the tree\n","        storage_folder: \"draupnir/src/draupnir/data/dataset_name\" or folder where the fasta file is located (recommended to put in a specific folder)\n","    out:\n","        if one_hot_encoding: where gap is [1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n","               Tensor with size: [Nsequences]x[Alignment length + 2]x[30] --> [[length,tree_position,dist_to_root,?,0,0,0...],[GIT vector],[aa1 one hot, phi, psi],[aa2 one hot, phi, psi],....[]]\n","        else: Amino acids are assigned numbers from 1-20, 0 means gap\n","               Tensor with size: [Nsequences]x[Alignment length + 3]x[30] --> [[length,tree_position,dist_to_root,0,0,0,0...],[GIT vector],[aa1 number, phi, psi],[aa2 number, phi, psi],....[]]\n","    \"\"\"\n","\n","    warnings.simplefilter('ignore', BiopythonWarning)\n","    one_hot_label= [\"onehot\" if one_hot_encoding else \"integers\"]\n","\n","    prot_info_dict = {}\n","    prot_aa_dict = {}\n","    if PDB_folder:# and not alignment_file:---> We allow to have sequences that have and don't have 3D structure\n","        print(\"Creating dataset from PDB files...\")\n","        prot_aa_dict,prot_info_dict = process_pdb_files(PDB_folder,aa_probs,pfam_dict,one_hot_encoding,min_len)\n","    #Remove duplicated sequences in both dictionaries\n","    if prot_aa_dict:\n","        fasta_file = \"{}/{}/{}.fasta\".format(storage_folder,name_file,name_file)\n","        print(\"Writing polypeptides to fasta file to {}\".format(fasta_file))\n","        with open(fasta_file, \"w\") as output_handle:\n","            pdb_records =[]\n","            for id,sequence in prot_aa_dict.items():\n","                pdb_record = SeqRecord(Seq(''.join(sequence)),\n","                                   id=str(id),\n","                                   description=\"\",\n","                                   annotations={\"molecule_type\": \"protein\"})\n","                pdb_records.append(pdb_record)\n","            SeqIO.write(pdb_records, output_handle, \"fasta\")\n","        output_handle.close()\n","    # Highlight: Align the polypeptides/sequences and write to a fasta angles_list file\n","    dict_alignment,alignment = infer_alignment(alignment_file,input_name_file=fasta_file,output_name_file=\"{}/{}/{}.mafft\".format(storage_folder,name_file,name_file))\n","    #calculate_pairwise_distance(name_file,alignment)\n","    alignment_file = [alignment_file if alignment_file else \"{}/{}/{}.mafft\".format(storage_folder,name_file,name_file)][0]\n","    #Highlight: checking that the selected number of probabilities is correct\n","    summary_aa_probs = [validate_sequence_alphabet(value) for key,value in dict_alignment.items()] #finds the alphabets of each of the sequences in the alignment, checks for dna\n","    aa_probs = max(aa_probs,max(summary_aa_probs)) #if the input aa_probs is different from those found, the aa_probs change. And also the aa substitution  matrix\n","    aa_names_dict = aminoacid_names_dict(aa_probs)\n","    #Highlight: If the aa sequences do not come from  PDB files, they come from an alignment file that needs to be processed\n","    #dict_alignment_2 = dict.fromkeys(dict_alignment.keys())\n","    not_aligned_seqs_from_alignment_file ={}\n","    for key,value in dict_alignment.items():\n","        aligned_seq = list(dict_alignment[key])\n","        #no_gap_indexes = np.where(np.array(aligned_seq) != \"-\")[0] + 2  # plus 2 in order to make the indexes fit in the final dataframe\n","        not_aligned_seq =list(filter(lambda a: a != \"-\", aligned_seq))\n","        seq_len = len(not_aligned_seq)\n","        git_vector = np.zeros(30) #fake git vector\n","        aa_info = np.zeros((seq_len + 2, 30))\n","        aa_info[0] = np.hstack([seq_len,[0]*29]) #first row contains some sequence length information\n","        aa_info[1] = git_vector #second row contains a git vector (never used but we could use it for something else)\n","        if one_hot_encoding:\n","            for index_a, aa_name in enumerate(not_aligned_seq):\n","                one_hot = np.zeros(aa_probs)\n","                index = aa_names_dict[aa_name]\n","                one_hot[index] = 1\n","                extra_space = 30-aa_probs\n","                aa_info[index_a+2] = np.hstack([one_hot,[0]*extra_space]) #extra_space = 10 for 20 aa_probs, 9 for 21 aa_probs, 8 for 22 aa_probs\n","        else:\n","            for index_aa, aa_name in enumerate(not_aligned_seq):\n","                index = aa_names_dict[aa_name]\n","                aa_info[index_aa+2] =np.hstack([index, [0]*29])\n","        not_aligned_seqs_from_alignment_file[key] = aa_info\n","        #dict_alignment_2[key] = aa_info\n","\n","    tree = infer_tree(alignment=alignment,\n","                      alignment_file_name=alignment_file,\n","                      name=name_file,\n","                      method=method,\n","                      tree_file_name=\"{}/{}/{}.tree\".format(storage_folder,name_file,name_file),\n","                      tree_file=tree_file,\n","                      storage_folder=\"{}/{}\".format(storage_folder,name_file))\n","    max_lenght = alignment.get_alignment_length()\n","\n","    #Highlight: Combining sequences in the alignment that have a PDB structure and those who don't\n","    if prot_info_dict: #Otherwise it does not loop over empty dictionaries and does nothing\n","        not_aligned_seqs_from_alignment_file.update((k, prot_info_dict[k]) for k, v in not_aligned_seqs_from_alignment_file.items() if k in prot_info_dict.keys())  #update the alignment keys with their homolog with pdb information. Update only those sequences in the alignment. Mafft and the IQ tree program might discard different proteins (for example they drop different identical proteins)\n","        Combined_dict = not_aligned_seqs_from_alignment_file\n","    else:\n","        Combined_dict = not_aligned_seqs_from_alignment_file\n","\n","\n","    if rename_internal_nodes:\n","        if name_file.startswith(\"simulations\"):\n","           tree = rename_tree_internal_nodes_simulations(tree,with_indexes=False)\n","        else:\n","           tree = rename_tree_internal_nodes(tree)\n","\n","    leafs_names = tree.get_leaf_names()\n","    pickle.dump(leafs_names,open('{}/{}/{}_Leafs_names_list.p'.format(storage_folder,name_file,name_file), 'wb'))\n","    # if len(leafs_names) <= 200:\n","    #     print(\"Rendering tree...\")\n","    #     render_tree(tree, \"{}/{}\".format(storage_folder,name_file), name_file)\n","    internal_nodes_names = [node.name for node in tree.traverse() if not node.is_leaf()]\n","\n","    ancestors_all =[]\n","    for node in tree.traverse():\n","        ancestors_node =[node.name.replace(\"'\",\"\")]+[node.dist] +[ancestor.name.replace(\"'\",\"\") for ancestor in node.get_ancestors()]\n","        ancestors_all.append(ancestors_node)\n","    length = max(map(len, ancestors_all))\n","    ancestors_info = np.array([xi + [None] * (length - len(xi)) for xi in ancestors_all])\n","\n","    tree_levelorder_names = np.asarray([node.name.replace(\"'\",\"\") for node in tree.traverse()])\n","\n","    tree_levelorder_dist =np.asarray([node.dist for node in tree.traverse()])\n","    #Add the index of the sequence in the tree to the seq length array info\n","    Dataset = np.zeros((len(Combined_dict), max_lenght + 1 + 1 +1, 30),dtype=object)  # 30 dim to accomodate git vectors. Careful with the +3 (to include git, seqlen and row/protein name)\n","    for i, (key,val) in enumerate(Combined_dict.items()):\n","        aligned_seq = list(dict_alignment[key].strip(\",\"))\n","        no_gap_indexes = np.where(np.array(aligned_seq) != \"-\")[0] + 3  # plus 3 in order to make the indexes fit in the final dataframe\n","        Dataset[i,0,0] = key.replace(\"'\",\"\") #row name/sequence name\n","        Dataset[i, 1:3] = Combined_dict[key][:2] #Insert seq len and git vector\n","        if name_file in [\"benchmark_randall\",\"benchmark_randall_original\",\"benchmark_randall_original_naming\"]:#their leaves have number names, so we keep them instead of changing them for the tree level order ones\n","            Dataset[i, 1, 1] = int(key.replace(\"'\", \"\"))\n","        elif name_file in [\"PF01038_lipcti_msa_fungi\"]:\n","            Dataset[i, 1, 1] = np.where(tree_levelorder_names == key.replace(\":\",\"_\"))[0][0]  # the node name will be its position in the tree\n","        else:\n","            Dataset[i,1,1] = np.where(tree_levelorder_names == key.replace(\"'\",\"\"))[0][0] #the node name will be its position in the tree\n","        Dataset[i, 1, 2] =  tree_levelorder_dist[Dataset[i,1,1]] #distance to the root? that's according to the documentation yes, but is different from the patristic distances\n","        Dataset[i, no_gap_indexes] = Combined_dict[key][2:]  # Assign the aa info (including angles) to those positions where there is not a gap\n","        if one_hot_encoding:\n","            #Highlight: no_gap indexes is not a boolean, we have to convert it\n","            gaps_mask = np.ones(max_lenght+3,np.bool)\n","            gaps_mask[no_gap_indexes] = False #do not keep the positions where there is not a gap\n","            gaps_mask[0] = False #not the first two rows\n","            gaps_mask[1] = False\n","            gaps_mask[2] = False\n","            Dataset[i,gaps_mask,0] = 1. #if one hot encoding the gaps with be assigned the first position in one hot encoding\n","\n","    #  Reconstruct the tree from the distance matrix\n","    print(\"Building patristic and cladistic matrices ...\")\n","    tree_save = pd.DataFrame(ancestors_info)\n","    #tree_save = pd.DataFrame({\"Nodes_Names\":tree_levelorder_names.tolist(),\"Distance_to_root\":tree_levelorder_dist.tolist()})\n","    tree_save.to_csv(\"{}/{}/{}_tree_levelorder_info.csv\".format(storage_folder,name_file,name_file),sep=\"\\t\")\n","    nodes_and_leafs_names = internal_nodes_names + leafs_names\n","    calculate_patristic_distance(name_file,Combined_dict,nodes_and_leafs_names,tree,tree_file,\"{}/{}\".format(storage_folder,name_file))\n","    calculate_closest_leaves(name_file,tree,\"{}/{}\".format(storage_folder,name_file))\n","    calculate_directly_linked_nodes(name_file, tree,\"{}/{}\".format(storage_folder,name_file))\n","    calculate_descendants(name_file,tree,\"{}/{}\".format(storage_folder,name_file))\n","\n","    print(\"Ready and saved!\")\n","    warnings.warn(\"Building clades (Collapses the original tree into monophyletic clades!)\")\n","    divide_into_monophyletic_clades(tree,\"{}/{}\".format(storage_folder,name_file),name_file)\n","    np.save(\"{}/{}/{}_dataset_numpy_aligned_{}.npy\".format(storage_folder,name_file,name_file,one_hot_label[0]), Dataset)\n","    max_lenght_not_aligned = max([int(sequence[0][0]) for idx,sequence in Combined_dict.items()]) #Find the largest sequence without being aligned\n","    print(\"Creating not aligned dataset...\")\n","    Dataset_not_aligned = np.zeros((len(Combined_dict), max_lenght_not_aligned +3, 30), dtype=object)  # 30 for future git vectors. Careful with the +3\n","    for i,(key,value) in enumerate(Combined_dict.items()):\n","        Dataset_not_aligned[i,0,0] = key\n","        Dataset_not_aligned[i, 1:3] = Combined_dict[key][:2] #Fill in the sequence lenght and the git vector\n","        if name_file in [\"benchmark_randall_original_naming\"]:\n","            Dataset[i, 1, 1] = int(key.replace(\"'\", \"\"))\n","        else:\n","            Dataset[i,1,1] = np.where(tree_levelorder_names == key.replace(\"'\",\"\"))[0][0] #position in the tree\n","        Dataset_not_aligned[i, 1, 2] =  tree_levelorder_dist[Dataset[i,1,1]]\n","        Dataset_not_aligned[i, 3:int(Combined_dict[key][0][0]) +3] = Combined_dict[key][2:] #Fill in the amino acids \"letters\"/\"numbers\" and their angles\n","        if one_hot_encoding:\n","            # #Highlight: no_gap indexes is not a boolean, we have to convert it\n","            # gaps_mask = np.ones(max_lenght+3,np.bool)\n","            # gaps_mask[no_gap_indexes] = False #do not keep the positions where there is not a gap\n","            # gaps_mask[0] = False #not the first two rows\n","            # gaps_mask[1] = False\n","            # gaps_mask[2] = False\n","            # Dataset[i,gaps_mask,0] = 1. #if one hot encoding the gaps with be assigned the first position in one hot encoding\n","            Dataset_not_aligned[i, (int(Combined_dict[key][0][0]) + 3):,0] = 1.\n","    np.save(\"{}/{}/{}_dataset_numpy_NOT_aligned_{}.npy\".format(storage_folder,name_file,name_file,one_hot_label[0]), Dataset_not_aligned)\n","\n","    return tree_file"],"metadata":{"id":"UfCaNzWp3OZ4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def infer_alignment(alignment_file,input_name_file,output_name_file):\n","    \"\"\"\n","    Reads and alignment or performs alignment using MAFFT [MAFFT multiple sequence alignment software version 7: improvements in performance and usability]. Returns a dictionary with the sequence name\n","    and the sequence and a biopython alignment object\n","    :param str or None alignment_file: path to pre-computed alignment to read\n","    :param str input_file_name: path to the file containing unaligned sequences,in fasta format\n","    :param str output_file_name: name of the file that will contain the aligned sequences\"\"\"\n","    # Align the polypeptides/sequences and write to a fasta file\n","    print(\"Analyzing alignment...\")\n","    if alignment_file: #The alignment file should contain the polypeptides of the PDB structures and sequences without structures\n","        print(\"Reading given alignment file ...\")\n","        # Read the aligned sequences\n","        alignment = AlignIO.read(\"{}\".format(alignment_file), \"fasta\")\n","        alignment_ids = []\n","        alignment_seqs = []\n","        for i,aligned in enumerate(alignment):\n","            alignment_ids.append(alignment[i].id)\n","            alignment_seqs.append(alignment[i].seq.strip(\"*\")) #Highlight: Remove stop codons\n","        dict_alignment = dict(zip(alignment_ids, alignment_seqs))\n","        return dict_alignment, alignment\n","    else:\n","        print(\"Using mafft to align...\")\n","        mafft_cline = MafftCommandline(input=input_name_file)\n","        stdout, stderr = mafft_cline()\n","        with open(output_name_file, \"w\") as handle:\n","            handle.write(stdout)\n","        handle.close()\n","\n","        # Read the aligned sequences\n","        alignment = AlignIO.read(output_name_file, \"fasta\")\n","        alignment_ids = [alignment[i].id for i, aligned in enumerate(alignment)]\n","        alignment_seqs = [alignment[i].seq for i, aligned in enumerate(alignment)]\n","        dict_alignment = dict(zip(alignment_ids, alignment_seqs))\n","        return dict_alignment, alignment\n"],"metadata":{"id":"Gth0v_UT9uUe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def aminoacid_names_dict(aa_probs):\n","  \"\"\" Returns an aminoacid associated to a integer value\n","  :param int aa_probs: amino acid probabilities, this number correlates to the number of different aa types in the input alignment\"\"\"\n","  if aa_probs == 21:\n","    aminoacid_names = {\"-\":0,\"R\":1,\"H\":2,\"K\":3,\"D\":4,\"E\":5,\"S\":6,\"T\":7,\"N\":8,\"Q\":9,\"C\":10,\"G\":11,\"P\":12,\"A\":13,\"V\":14,\"I\":15,\"L\":16,\"M\":17,\"F\":18,\"Y\":19,\"W\":20}\n","    return aminoacid_names\n","  if aa_probs == 22: #includes stop codons---> fix in Create blosum\n","    aminoacid_names = {\"-\":0,\"*\":0,\"R\":1,\"H\":2,\"K\":3,\"D\":4,\"E\":5,\"S\":6,\"T\":7,\"N\":8,\"Q\":9,\"C\":10,\"G\":11,\"P\":12,\"A\":13,\"V\":14,\"I\":15,\"L\":16,\"M\":17,\"F\":18,\"Y\":19,\"W\":20}\n","    return aminoacid_names\n","  elif aa_probs > 22:\n","    aminoacid_names = {\"-\":0,\"R\":1,\"H\":2,\"K\":3,\"D\":4,\"E\":5,\"S\":6,\"T\":7,\"N\":8,\"Q\":9,\"C\":10,\"G\":11,\"P\":12,\"A\":13,\"V\":14,\"I\":15,\"L\":16,\"M\":17,\"F\":18,\"Y\":19,\"W\":20,\"B\":21,\"Z\":22,\"X\":23}\n","    return aminoacid_names"],"metadata":{"id":"6f65BnyhyQ9D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def benchmark_randalls_dataset_train(name,args,storage_folder,aa_prob):\n","  \"\"\"Processing of the leaves dataset from \"An experimental phylogeny to benchmark ancestral sequence reconstruction\"\n","  :param str name: project dataset name\n","  :param int aa_prob: amino acid probabilities\"\"\"\n","  observed_nodes = [19,18,17,16,15,14,13,12,11,10,9,8,7,6,4,5,3,2,1] #I have this in a list for a series of past reasons\n","  sequences_file = \"/content/drive/MyDrive/bedford_lab/code/DRAUPNIR_ASR/data/benchmark_randall_original_naming/original_data/RandallExperimentalPhylogenyAASeqs.fasta\"\n","  #Select the sequences of only the observed nodes\n","  full_fasta = SeqIO.parse(sequences_file, \"fasta\")\n","  with open(\"{}/benchmark_randall_original_naming/original_data/Randall_Benchmark_Observed.fasta\".format(storage_folder), \"w\") as output_handle:\n","    observed_fasta = []\n","    for seq in full_fasta:\n","      if int(seq.id) in observed_nodes:\n","        observed_fasta.append(seq)\n","    SeqIO.write(observed_fasta, output_handle, \"fasta\")\n","  create_dataset(name,\n","                  one_hot_encoding=args.one_hot_encoded,\n","                  fasta_file = \"/content/drive/MyDrive/bedford_lab/code/DRAUPNIR_ASR/data/benchmark_randall_original_naming/Randall_Benchmark_Observed.fasta\",\n","                  alignment_file = \"/content/drive/MyDrive/bedford_lab/code/DRAUPNIR_ASR/data/benchmark_randall_original_naming/benchmark_randall_original_naming.mafft\",\n","                  tree_file = \"/content/drive/MyDrive/bedford_lab/code/DRAUPNIR_ASR/data/benchmark_randall_original_naming/RandallBenchmarkTree_OriginalNaming.tree\",\n","                  # fasta_file=\"{}/original_data/Randall_Benchmark_Observed.fasta\",\n","                  # alignment_file=\"{}/benchmark_randall_original.mafft\".format(storage_folder),\n","                  # tree_file=\"{}/RandallBenchmarkTree_OriginalNaming.tree\".format(storage_folder),\n","                  aa_probs=aa_prob,\n","                  rename_internal_nodes=False)"],"metadata":{"id":"cnPeAtx10bxz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def benchmark_randalls_dataset_test(settings_config,aa_probs=21):\n","  \"\"\"Processing of the internal nodes dataset from \"An experimental phylogeny to benchmark ancestral sequence reconstruction\n","  :param str scriptdir\n","  :param int aa_probs\"\"\"\n","  internal_nodes = [21,30,37,32,31,34,35,36,33,28,29,22,23,27,24,26,25]\n","  sequences_file = \"{}/original_data/RandallExperimentalPhylogenyAASeqs.fasta\".format(settings_config.data_folder)\n","  # Select the sequences of only the observed nodes\n","  full_fasta = SeqIO.parse(sequences_file, \"fasta\")\n","  aminoacid_names= aminoacid_names_dict(aa_probs)\n","  internal_fasta_dict = {}\n","  for seq in full_fasta:\n","    if int(seq.id) in internal_nodes:\n","      seq_numbers =[]\n","      for aa_name in seq.seq:\n","        #aa_number = int(np.where(np.array(aminoacid_names) == aa_name)[0][0]) + add_on\n","        aa_number = aminoacid_names[aa_name]\n","        seq_numbers.append(aa_number)\n","      internal_fasta_dict[int(seq.id)] = [seq.seq,seq_numbers]\n","  max_length = max([int(len(sequence[0])) for idx,sequence in internal_fasta_dict.items()]) #225\n","\n","  dataset = np.zeros((len(internal_fasta_dict), max_length + 1 + 1, 30),dtype=object)  # 30 dim to accomodate git vectors. Careful with the +2 (to include git, seqlen)\n","  for i, (key,val) in enumerate(internal_fasta_dict.items()):\n","    # aligned_seq = list(alignment[i].seq.strip(\",\")) # I don't think this made sense, cause files could be in wrong order?\n","    aligned_seq = list(internal_fasta_dict[key][0].strip(\",\"))\n","    no_gap_indexes = np.where(np.array(aligned_seq) != \"-\")[0] + 2  # plus 2 in order to make the indexes fit in the final dataframe\n","    dataset[i, 0,0] = len(internal_fasta_dict[key][1]) #Insert seq len and git vector\n","    dataset[i,0,1] = key #position in the tree\n","    dataset[i, 0, 2] =  0 #fake distance to the root\n","    dataset[i, no_gap_indexes,0] = internal_fasta_dict[key][1] # Assign the aa info (including angles) to those positions where there is not a gap\n","\n","  return dataset, internal_nodes"],"metadata":{"id":"SyEM3IeOxqCI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_draupnir_dataset(name,use_custom,script_dir,args,build=False,fasta_file=None,tree_file=None,alignment_file=None):\n","  \"\"\"In:\n","  :param str name: Dataset name\n","  :param bool use_custom: True (uses a custom dataset, located in datasets/custom/\"folder_name\" ) or False (uses a Draupnir default dataset (used in the publication))\n","  :param str script_dir: Working directory of Draupnir #TODO: remove\n","  :param bool build: Activates the construction of the dataset, might take a while if it requires to build tree, so it's recommended to use the pre-saved files\n","  :param str or None fasta_file: Path to NOT aligned sequences\n","  :param str or None tree_file: Path to Newick tree, format 1 in ete3\n","  :param str or None alignment_file: Path to pre-aligned sequences\n","  :returns namedtuple build_config:\n","      :str alignment-file:\n","      :bool use_ancestral: True (patristic_matrix_train = patristic_matrix_full (leaves + ancestors)), False (patristic_matrix_train = patristic_matrix) otherwise we remove the ancestral nodes from patristic_matrix_train. Necessary for some datasets\n","      :int n_test: percentage of train/leaves sequences to be used as test, i-e n_test = 20 ---> 20% leaves will be th etest datasets\n","      build_graph: make a graph for CNN #TODO: remove?\n","      :int aa_prob: Number of amino acid probabilities (21 or 24), depends on the different types of amino acids in the sequence alignment\n","      :bool triTSNE: Whether to plot TSNE in 3D (True) or not #TODO: Remove\n","      :bool leaves_testing: True (uses all the leaf's evolutionary distances for training, it only observes (n-n_test) leafsequences. USE WITH n_test), False (uses all the leaf's evolutionary distances for training\n","                          and observes all the leaf sequences. Use with datasets without ancestors for testing, only generate sequences).\n","      \"\"\"\n","  BuildConfig = namedtuple('BuildConfig',['alignment_file','use_ancestral','n_test','build_graph',\"aa_prob\",\"triTSNE\",\"leaves_testing\",\"script_dir\",\"no_testing\"],module=\"build_config\") #__name__ + \".namespace\"\n","  SettingsConfig = namedtuple(\"SettingsConfig\", [\"one_hot_encoding\", \"model_design\", \"aligned_seq\",\"data_folder\",\"full_name\",\"tree_file\"],module=\"settings_config\")\n","  if args.one_hot_encoded:\n","    warnings.warn(\"Draupnir was constructed to be used with integers for Categorical likelihood,not OneHotCategorical. And blosum-encoding for the guide. You can build the one-hot-encoded dataset for other purposes\")\n","\n","  #script_dir = os.path.dirname(os.path.abspath(__file__))\n","  if not use_custom:\n","    warnings.warn(\"You have selected a pre-defined dataset, if not present, it will be downloaded. Otherwise set args.use_custom to True\")\n","    root_sequence_name = available_datasets()[0][name]\n","    full_name = available_datasets()[1][name]\n","    storage_folder = os.path.abspath(os.path.join(os.path.dirname(\"/content/drive/MyDrive/bedford_lab/code/DRAUPNIR_ASR/\"), \"data\")) #changed from \"datasets/default\"\n","    dir_name = '{}/{}'.format(storage_folder,name)\n","    dict_urls = {\n","      \"aminopeptidase\":\"https://drive.google.com/drive/folders/1fLsOJbD1hczX15NW0clCgL6Yf4mnx_yl?usp=sharing\",\n","      \"benchmark_randall_original_naming\":\"https://drive.google.com/drive/folders/1oE5-22lqcobZMIguatOU_Ki3N2Fl9b4e?usp=sharing\",\n","      \"Coral_all\":\"https://drive.google.com/drive/folders/1IbfiM2ww5PDcDSpTjrWklRnugP8RdUTu?usp=sharing\",\n","      \"Coral_Faviina\":\"https://drive.google.com/drive/folders/1Ehn5xNNYHRu1iaf7vS66sbAESB-dPJRx?usp=sharing\",\n","      \"PDB_files_Draupnir_PF00018_116\":\"https://drive.google.com/drive/folders/1YJDS_oHHq-5qh2qszwk-CucaYWa9YDOD?usp=sharing\",\n","      \"PDB_files_Draupnir_PF00400_185\": \"https://drive.google.com/drive/folders/1LTOt-dhksW1ZsBjb2uzi2NB_333hLeu2?usp=sharing\",\n","      \"PF00096\":\"https://drive.google.com/drive/folders/103itCfxiH8jIjKYY9Cvy7pRGyDl9cnej?usp=sharing\",\n","      \"PF00400\":\"https://drive.google.com/drive/folders/1Ql10yTItcdX93Xpz3Oh-sl9Md6pyJSZ3?usp=sharing\",\n","      \"SH3_pf00018_larger_than_30aa\":\"https://drive.google.com/drive/folders/1Mww3uvF_WonpMXhESBl9Jjes6vAKPj5f?usp=sharing\",\n","      \"simulations_blactamase_1\":\"https://drive.google.com/drive/folders/1ecHyqnimdnsbeoIh54g2Wi6NdGE8tjP4?usp=sharing\",\n","      \"simulations_calcitonin_1\":\"https://drive.google.com/drive/folders/1jJ5RCfLnJyAq0ApGIPrXROErcJK3COvK?usp=sharing\",\n","      \"simulations_insulin_2\":\"https://drive.google.com/drive/folders/1xB03AF_DYv0EBTwzUD3pj03zBcQDDC67?usp=sharing\",\n","      \"simulations_PIGBOS_1\":\"https://drive.google.com/drive/folders/1KTzfINBVo0MqztlHaiJFoNDt5gGsc0dK?usp=sharing\",\n","      \"simulations_sirtuins_1\":\"https://drive.google.com/drive/folders/1llT_HvcuJQps0e0RhlfsI1OLq251_s5S?usp=sharing\",\n","      \"simulations_src_sh3_1\":\"https://drive.google.com/drive/folders/1tZOn7PrCjprPYmyjqREbW9PFTsPb29YZ?usp=sharing\",\n","      \"simulations_src_sh3_2\":\"https://drive.google.com/drive/folders/1ji4wyUU4aZQTaha-Uha1GBaYruVJWgdh?usp=sharing\",\n","      \"simulations_src_sh3_3\":\"https://drive.google.com/drive/folders/13xLOqW2ldRNm8OeU-bnp9DPEqU1d31Wy?usp=sharing\"\n","\n","        }\n","    #download=False #TODO: Remove\n","    # if os.path.isdir(dir_name):\n","    #   if not os.listdir(dir_name):\n","    #     print(\"Directory is empty\")\n","    #     #if download:\n","    #     # os.remove(dir_name)\n","    #     print(\"Data directory is missing. Downloading, this might take a while. If you see an error like \\n\"\n","    #           \" 'Cannot retrieve the public link of the file. You may need to change the permission to <Anyone with the link>, or have had many accesses', \\n\"\n","    #           \"just wait, too many requests have been made to the google drive folder \\n\"\n","    #           \"Otherwise just download the data sets manually from the google drive urls : \\n {}\".format(\n","    #         dict_urls[name]))\n","    #     gdown.download_folder(dict_urls[name], output='{}/{}'.format(storage_folder, name), quiet=True,\n","    #                           use_cookies=False, remaining_ok=True)\n","    #     # else:\n","    #     #     pass\n","\n","\n","  if name == \"benchmark_randall_original_naming\":\n","    alignment_file = \"{}/{}/benchmark_randall_original_naming.mafft\".format(storage_folder,name)\n","    tree_file = \"{}/{}/RandallBenchmarkTree_OriginalNaming.tree\".format(storage_folder,name)\n","    build_config = BuildConfig(alignment_file=alignment_file,\n","                                use_ancestral=True, n_test=0,\n","                                build_graph=True,aa_prob=21,\n","                                triTSNE=False,leaves_testing=False,script_dir=script_dir,no_testing=False)\n","    if build:\n","      benchmark_randalls_dataset_train(name, args,storage_folder,aa_prob=21)\n","\n","  settings_config = SettingsConfig(one_hot_encoding=args.one_hot_encoded,\n","                             model_design=\"GP_VAE\",\n","                             aligned_seq=True,\n","                             data_folder=\"{}/{}\".format(storage_folder,name), #[\"{}\".format(storage_folder) if use_custom else \"{}/{}\".format(storage_folder,name)][0]\n","                             full_name=full_name,\n","                             tree_file=tree_file)\n","  return build_config,settings_config, root_sequence_name"],"metadata":{"id":"x3kkWGNDwqFX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def validate_sequence_alphabet(seq):\n","    \"\"\"\n","    Checks that the sequences from an alignment only contains values from one of the protein alphabets (protein21 or protein21plus). Reject DNA or RNA sequences\n","    :param str seq: sequence of characters\n","    \"\"\"\n","    alphabets = {'dna': re.compile('^[acgtn]*$', re.I),\n","             'protein21': re.compile('^[-acdefghiklmnpqrstvwy]*$', flags=re.IGNORECASE),\n","            'protein21plus': re.compile('^[-acdefghiklmnpqrstvwybzx]*$', flags= re.IGNORECASE)}\n","\n","    if alphabets[\"dna\"].search(str(seq)) is not None: raise ValueError(\"Please use amino acids in your sequences, accepted alphabets are protein21: -acdefghiklmnpqrstvwy or protein21plus: -*acdefghiklmnpqrstvwybzx\")\n","\n","    if alphabets[\"protein21\"].search(str(seq)) is not None:\n","        aa_probs = 21\n","        return aa_probs\n","    if alphabets[\"protein21plus\"].search(str(seq)) is not None:\n","        aa_probs = 24\n","        return aa_probs\n","    else:\n","        raise ValueError(\"Your sequences contain not allowed characters. Available alphabets are: {protein21}: -acdefghiklmnpqrstvwy or {protein21plus} -*acdefghiklmnpqrstvwybzx. If your sequence contains stop codons perhaps you can trim them.\")"],"metadata":{"id":"K6hphVcBPQJO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def infer_tree(alignment, alignment_file_name,name,method=None,tree_file_name=None,tree_file=None,storage_folder=\"\"):\n","    \"\"\" Performs tree inference or reads an input given tree, returns an ete3 tree formated tree\n","    :param biopython alignment alignment: biopython alignment class\n","    :param str alignment_file_name: path to alignment file\n","    :param str name: dataset project name\n","    :param str method: tree inference method (if tree is not given)\n","    :param str tree_file_name: name to give to the tree file\n","    .param str tree_file:path to a possible given tree in newick format 1\n","    :param str storage_folder: folder where to store the results of the tree inference\n","\n","    \"\"\"\n","    if tree_file:\n","        print(\"Using given tree file...\")\n","        tree = TreeEte3(tree_file,format=1,quoted_node_names=True)\n","        return tree\n","\n","    else:\n","        # Pairwise distance matrix\n","        print(\"Building distance matrices and {} tree...\".format(method))\n","        if len(alignment) < 200 and method in [\"nj\",\"nj_rooted\",\"upgma\"]:\n","            calculator = DistanceCalculator('blosum62')  # DNA ---> Identity// Protein ---> blosum62\n","            distance_matrix_cal = calculator.get_distance(alignment)\n","            distance_matrix_cal_pandas = convert_to_pandas(distance_matrix_cal)\n","            distance_matrix_cal_pandas.to_csv(\"{}/{}_distance_matrix.csv\".format(storage_folder,name))\n","            #https://stackoverflow.com/questions/30247359/how-does-biopython-determine-the-root-of-a-phylogenetic-tree\n","            if method == \"nj\":\n","                print(\"Tree inference via Neighbour Joining NOT rooted method...\")\n","                constructor = DistanceTreeConstructor(method=\"nj\")\n","                tree = constructor.nj(distance_matrix_cal)\n","                tree = to_ete3(tree)\n","                return tree\n","            elif method == \"nj_rooted\":\n","                print(\"Tree inference via Neighbour Joining with additional rooting method...\")\n","                constructor = DistanceTreeConstructor(method=\"nj\")\n","                tree = constructor.nj(distance_matrix_cal)\n","                tree = to_ete3(tree)\n","                # Making a root:\n","                sequence_0, sequence_1 = tree_pair_for_rooting(distance_matrix_cal_pandas)\n","                tree.set_outgroup(tree & sequence_0)\n","                # ancestor = tree.get_common_ancestor(sequence_0, sequence_1)\n","                # tree.set_outgroup(ancestor)\n","                return tree\n","            elif method == \"upgma\":\n","                print(\"Tree inference via Upgma rooted method...\")\n","                constructor = DistanceTreeConstructor(method=\"upgma\") # nj method is unrooted in biopython. upgma is rooted\n","                tree = constructor.upgma(distance_matrix_cal)\n","                tree = to_ete3(tree)\n","                return tree\n","        elif method == \"iqtree\":\n","            print(\"Iqtree ML method...\")\n","            alignment_f = [alignment_file_name if alignment_file_name else \"{}/{}.mafft\".format(storage_folder,name)][0]\n","            tree_file_name = alignment_f.split(\".\")[0] + \".treefile\"\n","\n","            if not os.path.exists(tree_file_name):\n","                #-o\tSpecify an outgroup taxon name to root the tree. The output tree in .treefile will be rooted accordingly. DEFAULT: first taxon in alignment\n","                taxon_root = False\n","                if taxon_root:\n","                    root=21\n","                    subprocess.run(args=[\"iqtree\",\"-s\",alignment_f.split(\".\")[0],\"--aln\",alignment_f,\"-nt\",\"AUTO\",\"-o\",root],stderr=sys.stderr, stdout=sys.stdout)\n","                else:\n","                    subprocess.run(args=[\"iqtree\",\"-s\",alignment_f.split(\".\")[0],\"--aln\",alignment_f,\"-nt\",\"AUTO\"],stderr=sys.stderr, stdout=sys.stdout)\n","                os.remove(alignment_f + \".log\")\n","                os.remove(alignment_f + \".bionj\")\n","                os.remove(alignment_f + \".ckp.gz\")\n","                os.remove(alignment_f + \".model.gz\")\n","            distance_matrix_cal = pd.read_csv(alignment_f+\".mldist\", sep=\"\\\\s+\", skiprows=1, header=None)\n","            distance_matrix_cal.columns = [\"rows\"] + distance_matrix_cal.iloc[:,0].to_list()\n","            distance_matrix_cal.set_index(\"rows\", inplace=True)\n","            distance_matrix_cal.index.name = \"\"\n","            distance_matrix_cal.to_csv(\"{}/{}_distance_matrix.csv\".format(storage_folder,name))\n","            tree = TreeEte3(alignment_f+ \".treefile\")\n","            return tree\n","        elif method == \"rapidnj\":\n","            print(\"Using Rapidnj to build NOT rooted tree...\")\n","            tree_file_name = [\"{}/{}.tree\".format(storage_folder,name) if not tree_file_name else tree_file_name][0]\n","            alignment_f = [alignment_file_name if alignment_file_name else \"{}/{}.mafft\".format(storage_folder,name)][0]\n","            with open(tree_file_name, \"w\") as tree_file_out:\n","                subprocess.run(args=[\"rapidnj\",alignment_f, \"-i\", \"fa\"], stdout=tree_file_out)\n","            tree_file_out.close()\n","            tree = TreeEte3(tree_file_name)\n","            return tree"],"metadata":{"id":"hzOAk0oUPoNq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def render_tree(tree,storage_folder,name_file):\n","    \"\"\"Function to render an ete3 tree into an image\n","    :param ete3-tree tree: Ete3 tree object\n","    :param str storage_folder: path to folder where to store the results\n","    :param name_file: data set project name\"\"\"\n","    ts = TreeStyle()\n","    ns = NodeStyle()\n","    #Make thicker lines\n","    ns[\"vt_line_width\"] = 5\n","    ns[\"hz_line_width\"] = 5\n","    # Do not add leaf names automatically\n","    ts.show_leaf_name = False\n","    # Use my custom layout\n","    ts.layout_fn = my_layout\n","    #print the branch lengths\n","    ts.show_branch_length = True\n","    for n in tree.traverse():\n","            n.set_style(ns)\n","    try:\n","        tree.render(\"{}/return_{}.png\".format(storage_folder,name_file),w=1000, units=\"mm\",tree_style=ts)\n","    except:\n","        tree.render(\"{}/return_{}.png\".format(storage_folder,name_file), w=1000, units=\"mm\")"],"metadata":{"id":"lKNptFLPQ_CJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def my_layout(node):\n","    \"\"\"Ete3 layout that adds the internal nodes names. It is a plug-in for rendering tree images\n","    :param ete3-node node: node from an ete3 tree\"\"\"\n","    if node.is_leaf():\n","        # If terminal node, draws its name\n","        name_face = AttrFace(\"name\",fsize=8,fgcolor=\"blue\")\n","    else:\n","        # If internal node, draws label with smaller font size\n","        name_face = AttrFace(\"name\", fsize=8,fgcolor=\"red\")\n","    # Adds the name face to the image at the preferred position\n","    faces.add_face_to_node(name_face, node, column=0, position=\"branch-right\")"],"metadata":{"id":"NhRgGs5XThRy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calculate_patristic_distance(name_file,combined_dict,nodes_and_leafs_names,tree,tree_file, storage_folder):\n","    \"\"\"Calculates the patristic distances or branch lengths across the nodes in a tree. It also saves the tree in different formats needed for benchmarking etc\n","    :param str name_file: data set project name\n","    :param dict combined_dict #TODO: Remove?\n","    :param list nodes_and_leafs_names: tree nodes in tree-level order stored in a list\n","    :param ete3-tree tree: ete3 object containing tree\n","    :param str tree_file: path to the stored tree file\n","    :param str storage_folder: folder where to store the results\n","    \"\"\"\n","\n","    n_seqs = len(combined_dict)\n","    #work_dir = os.path.dirname(os.path.abspath(__file__))\n","    work_dir = \"\"\n","    if n_seqs > 200:\n","        print(\"Dataset larger than 200 sequences: Using R script for patristic distances (cladistic matrix is NOT available)!\")\n","        warnings.warn(\"Dataset larger than 200 sequences: Requires R and the ape library\")\n","        command = 'Rscript'\n","        #path2script = '/home/lys/Dropbox/PhD/DRAUPNIR/Calculate_Patristic.R'\n","        path2script = \"Calculate_Patristic.R\"\n","        if tree_file:\n","            new_tree = work_dir +tree_file.split(\".\")[0]+\".newick\"\n","            new_tree_format8 = work_dir  + tree_file.split(\".\")[0] + \".format8newick\"\n","            new_tree_format6 = work_dir  + tree_file.split(\".\")[0] + \".format6newick\"\n","            new_tree_format7 = work_dir  + tree_file.split(\".\")[0] + \".format7newick\"\n","        else:\n","            new_tree = work_dir + \"{}/{}.newick\".format(storage_folder,name_file)\n","            new_tree_format8 = work_dir + \"{}/{}.format8newick\".format(storage_folder,name_file)\n","            new_tree_format6 = work_dir + \"{}/{}.format6newick\".format(storage_folder,name_file)\n","            new_tree_format7 = work_dir + \"{}/{}.format7newick\".format(storage_folder,name_file)\n","\n","        tree.write(outfile=new_tree_format8, format=8,format_root_node=True) # format 8 all nodes names\n","        tree.write(outfile=new_tree_format6, format=6,format_root_node=True)\n","        tree.write(outfile=new_tree_format7, format=7,format_root_node=True) #all nodes names + branch lengths\n","        tree.write(outfile=new_tree,format=1) #save the renamed tree, format 9 to not save the internal nodes names\n","        patristic_file = \"{}/{}_patristic_distance_matrix.csv\".format(storage_folder,name_file)\n","        if not os.path.exists(patristic_file):\n","            # Build subprocess command\n","            subprocess.check_call([command,path2script,new_tree,patristic_file])\n","        else:\n","            print(\"Patristic matrix already exists at {}, not calculated. Delete it otherwise\".format(patristic_file))\n","        #Highlight: PHYLOCOM\n","        # # #Highlight:Transform the file to Nexus format\n","        # new_tree = tree_file.split(\".\")[0]+\".newick\"\n","        # new_tree_filename = ntpath.basename(new_tree)\n","        # working_directory = os.path.dirname(os.path.abspath(new_tree))\n","        # tree.write(outfile=new_tree,format=1) #save the renamed tree, format 9 to not save the internal nodes names\n","        # patristic_file = \"Datasets_Folder/Patristic_distance_matrix_{}.txt\".format(name_file)\n","        # with open(patristic_file, \"w\") as patristic_dist_out:\n","        #     subprocess.run(args=[\"phylocom\",\"phydist\", \"-f\", new_tree_filename],stderr=sys.stderr, stdout=patristic_dist_out,cwd=working_directory)\n","        # patristic_matrix = pd.read_csv(patristic_file,sep=\"\\t\",index_col=0)\n","        # patristic_matrix.to_csv(\"Datasets_Folder/Patristic_distance_matrix_{}.csv\".format(name_file),index_label=\"rows\")\n","\n","    else:\n","        if tree_file:\n","            new_tree = work_dir + \"/\" + tree_file.split(\".\")[0] +\".newick\"\n","            new_tree_format8 = work_dir + \"/\" + tree_file.split(\".\")[0] + \".format8newick\"\n","            new_tree_format6 = work_dir + \"/\" + tree_file.split(\".\")[0] + \".format6newick\"\n","            new_tree_format7 = work_dir + \"/\" + tree_file.split(\".\")[0] + \".format7newick\"\n","        else:\n","            new_tree = work_dir + \"/{}/{}.newick\".format(storage_folder,name_file)\n","            new_tree_format8 = work_dir + \"/{}/{}.format8newick\".format(storage_folder,name_file)\n","            new_tree_format6 = work_dir + \"/{}/{}.format6newick\".format(storage_folder,name_file)\n","            new_tree_format7 = work_dir + \"/{}/{}.format7newick\".format(storage_folder,name_file)\n","\n","\n","        tree.write(outfile=new_tree_format8, format=8,format_root_node=True)\n","        tree.write(outfile=new_tree_format6, format=6,format_root_node=True)\n","        tree.write(outfile=new_tree_format7, format=7,format_root_node=True)\n","        tree.write(outfile=new_tree,format=1)  # save the renamed tree, format 9 to not save the internal nodes names. format 8 all nodes names\n","\n","        n_elements = len(nodes_and_leafs_names)\n","        I = pd.Index(nodes_and_leafs_names, name=\"rows\")\n","        C = pd.Index(nodes_and_leafs_names, name=\"columns\")\n","        patristic_matrix = pd.DataFrame(data=np.zeros((n_elements, n_elements)), index=I, columns=C)\n","        cladistic_matrix = pd.DataFrame(data=np.zeros((n_elements, n_elements)), index=I, columns=C)\n","        if not os.path.exists(\"{}/{}_patristic_distance_matrix.csv\".format(storage_folder,name_file)):\n","            for i, t1 in enumerate(nodes_and_leafs_names):\n","                for j, t2 in enumerate(list(nodes_and_leafs_names)[i + 1:]):\n","                    cladistic_matrix.loc[[t1], [t2]] = tree.get_distance(t1, t2, topology_only=True)\n","                    patristic_matrix.loc[[t1], [t2]] = tree.get_distance(t1, t2, topology_only=False)\n","            cladistic_matrix.to_csv(\"{}/{}_cladistic_distance_matrix.csv\".format(storage_folder,name_file))\n","            patristic_matrix.to_csv(\"{}/{}_patristic_distance_matrix.csv\".format(storage_folder,name_file))\n","        else:\n","            print(\"Patristic matrix file already exists, not calculated\")"],"metadata":{"id":"umVqmznYYetC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calculate_closest_leaves(name,tree,storage_folder):\n","    \"\"\" Creates a dictionary that contains the closest leave to an internal node {internal_node:leave}\n","    :param str name: data set project name\n","    :param ete3-tree tree: Ete3 tree class object\n","    :param str storage_folder: folder where to dump the output\n","    \"\"\"\n","    closest_leaves_dict=defaultdict() #closest leave to an internal node\n","    for node in tree.traverse():\n","        if not node.is_leaf(): #if it's an internal node\n","            terminal_node = all(node.is_leaf() for node in node.get_children())\n","            if terminal_node:\n","                closest_leaves_dict[node.name] = [node.name for node in node.get_children()]\n","            else:\n","                closest_leaves_dict[node.name] = [node.get_closest_leaf()[0].name]\n","    pickle.dump(closest_leaves_dict, open('{}/{}_Closest_leaves_dict.p'.format(storage_folder,name), 'wb'),protocol=pickle.HIGHEST_PROTOCOL)"],"metadata":{"id":"g0pqbaIcYlRs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calculate_directly_linked_nodes(name,tree,storage_folder):\n","    \"\"\"Creates a dictionary that contains the 2 children nodes directly linked to a node (not all the children from that node) {node:children}\n","    :param str name: data set project name\n","    :param ete3-tree tree: Ete3 tree class object\n","    :param str storage_folder: folder where to dump the output\"\"\"\n","    closest_children_dict=defaultdict()\n","    for node in tree.traverse():\n","        closest_children_dict[node.name] = [node.name for node in node.get_children()]\n","    pickle.dump(closest_children_dict, open('{}/{}_Closest_children_dict.p'.format(storage_folder,name), 'wb'),protocol=pickle.HIGHEST_PROTOCOL)"],"metadata":{"id":"JIoIm5pOYqMQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calculate_descendants(name,tree,storage_folder):\n","    \"\"\"Creates a dictionary that contains all the internal nodes and leaves that descend from that internal node {internal_node:descendants}\n","    :param str name: data set project name\n","    :param ete3-tree tree: Ete3 tree class object\n","    :param str storage_folder: folder where to dump the output\"\"\"\n","    closest_descendants_dict = defaultdict(lambda: defaultdict())\n","    for node in tree.traverse():\n","        if not node.is_leaf():\n","            descendant_leaves = []\n","            descendant_internal = [node.name]\n","            for descendant in node.iter_descendants():\n","                if descendant.is_leaf():\n","                    descendant_leaves.append(descendant.name)\n","                else:\n","                    descendant_internal.append(descendant.name)\n","            closest_descendants_dict[node.name][\"internal\"] = descendant_internal\n","            closest_descendants_dict[node.name][\"leaves\"] = descendant_leaves\n","    dill.dump(closest_descendants_dict, open('{}/{}_Descendants_dict.p'.format(storage_folder,name), 'wb'))#,protocol=pickle.HIGHEST_PROTOCOL)"],"metadata":{"id":"eHPssL-vYq9J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def divide_into_monophyletic_clades(tree,storage_folder,name):\n","    \"\"\"\n","    Divides the tree into monophyletic clades:\n","    See https://www.mun.ca/biology/scarr/Taxon_types.html\n","    Implementation based on: https://www.biostars.org/p/97409/\n","\n","    The reasonable clade division criteria seems to group all those nodes whose distance to the internal is lower to the overall average distance from each leaf to the root\n","\n","    :param ete3-tree tree: ete3 tree class\n","    :param str storage_folder: folder where to store the results, in this case a dictionary containing {\"clade_number\": [nodes list]}\n","    :name str name: name of the data set project\n","\n","    \"\"\"\n","\n","    def mean(array):\n","        \"\"\"Calculates branch length average\"\"\"\n","        return sum(array) / float(len(array))\n","\n","    def cache_distances(tree):\n","        \"\"\"Precalculate distances of all nodes to the root\"\"\"\n","        node2rootdist = {tree: 0}\n","        for node in tree.iter_descendants('preorder'):\n","            node2rootdist[node] = node.dist + node2rootdist[node.up]\n","        return node2rootdist\n","\n","    def build_clades(tree,name):\n","        \"\"\"When a clustering condition is met, it collapses the tree at that node to unify all the leaves in that cluster into 'one' leaves. After, we read\n","        the collapsed tree into a dictionary that contains {clade number:{\"internal\":[nodes numbers],\"leaves\":[node numbers]}}\"\"\"\n","        # cache the tip content of each node to reduce the number of times the tree is traversed\n","        node2tips = tree.get_cached_content()\n","        root_distance = cache_distances(tree)  # distances of each of the nodes to the root\n","        average_root_distance = mean(root_distance.values())\n","        std_root_distance = statistics.stdev(root_distance.values())\n","        n_leaves = len(tree.get_leaves())\n","        #TODO: automatize clustering condition\n","        if n_leaves >= 100 or name.endswith(\"_subtree\"):\n","            if name in [\"PF00096\",\"PF00400\"]:\n","                clustering_condition = average_root_distance #+ 0.3*std_root_distance\n","            else:\n","                clustering_condition = average_root_distance -2*std_root_distance\n","        elif name in [\"Coral_all\",\"Coral_Faviina\",\"SH3_pf00018_larger_than_30aa\"] or \"calcitonin\" in name:\n","            clustering_condition = average_root_distance - std_root_distance\n","        else:\n","            clustering_condition = average_root_distance\n","\n","        for node in tree.get_descendants('preorder'):\n","            if not node.is_leaf():  # for internal nodes\n","                avg_distance_to_tips = mean([root_distance[tip] - root_distance[node] for tip in node2tips[node]])  # average distance from the internal node to all it's possible derived leaves\n","                if avg_distance_to_tips < clustering_condition:\n","                    #node.name += ' COLLAPSED avg_d:%g {%s}' % (avg_distance_to_tips, ','.join([tip.name for tip in node2tips[node]]))\n","                    node.name += ' COLLAPSED avg_d:%g leaves:{%s} internal:{%s}' % (avg_distance_to_tips, ','.join([tip.name for tip in node2tips[node]]),','.join([internal.name for internal in node.iter_descendants() if not internal.is_leaf()]))\n","                    node.add_features(collapsed=True)\n","                    node.img_style['draw_descendants'] = False\n","\n","        for n in tree.search_nodes(collapsed=True):\n","            for child in n.get_children():\n","                child.detach()\n","        #print(tree.get_ascii(show_internal=True))\n","        i = 0\n","        clade_dict_all = defaultdict(lambda: defaultdict())\n","        for n in tree.traverse():\n","                if n.is_leaf() and \"COLLAPSED\" in n.name: #collapsed leaf (it is a clade on it's own)\n","                    clade_names_leaves = n.name[n.name.find(\"leaves:{\") + 8:n.name.find(\"}\")].split(\",\")\n","                    clade_names_internal = [n.name.split(\" \")[0]]\n","                    clade_names_internal += n.name[n.name.find(\"internal:{\") + 10:].strip(\"}\").split(\",\")\n","                    clade_dict_all[\"Clade_{}\".format(i)][\"leaves\"] = set(clade_names_leaves)  # remove duplicates\n","                    clade_dict_all[\"Clade_{}\".format(i)][\"internal\"] = list(filter(None,set(clade_names_internal))) #sometimes the  node strings are empty\n","                    i += 1\n","                elif not n.is_leaf(): #if the node is internal\n","                    clade_names_leaves = []\n","                    clade_names_internal = []\n","                    clade_names_internal += [n.name]\n","                    for descendant in n.iter_descendants():\n","                        if descendant.is_leaf():\n","                            if \"{\" not in descendant.name: #it was a pure leaf\n","                                clade_names_leaves += [descendant.name]\n","                            else: #is a collapsed leave\n","                                clade_names_leaves += descendant.name[descendant.name.find(\"leaves:{\")+8:descendant.name.find(\"}\")].split(\",\")\n","                                clade_names_internal += [descendant.name.split(\" \")[0]]\n","                                clade_names_internal += descendant.name[descendant.name.find(\"internal:{\")+10:].strip(\"}\").split(\",\")\n","                        else: #add the internal node also to it's clade\n","                            clade_names_internal += [descendant.name]\n","\n","                    clade_dict_all[\"Clade_{}\".format(i)][\"leaves\"] = set(clade_names_leaves) #remove duplicates\n","                    clade_dict_all[\"Clade_{}\".format(i)][\"internal\"] = list(filter(None,set(clade_names_internal))) #sometimes the  node strings are empty\n","                    i += 1\n","                else:#Non collapsed leaves\n","                    pass\n","        clade_dict_leaves = defaultdict()\n","        i = 0\n","        for n in tree.traverse(\"preorder\"):\n","            if n.is_leaf():\n","                if \"{\" not in n.name:\n","                    clade_names_leaves = [n.name]\n","                else:\n","                    clade_names_leaves = n.name[n.name.find(\"leaves:{\") + 8:n.name.find(\"}\")].split(\",\")\n","                clade_dict_leaves[\"Clade_{}\".format(i)] = clade_names_leaves\n","                i += 1\n","\n","        return clade_dict_leaves,clade_dict_all\n","\n","    clade_dict_leaves,clade_dict_all = build_clades(tree,name)\n","    #Highlight: clades_dict_all contains each clade's internal and leaves nodes, clades_dict_leaves only contains the leaves of each clade\n","\n","    dill.dump(clade_dict_all, open('{}/{}_Clades_dict_all.p'.format(storage_folder,name), 'wb'))#,protocol=pickle.HIGHEST_PROTOCOL)\n","    pickle.dump(clade_dict_leaves, open('{}/{}_Clades_dict_leaves.p'.format(storage_folder,name), 'wb'),protocol=pickle.HIGHEST_PROTOCOL)"],"metadata":{"id":"J5iB2G1MZE4V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !cd /usr/local/lib/python3.11/dist-packages/PyQt5 && ldd *.so | grep 'not found'| sort | uniq"],"metadata":{"id":"XYoZOF8iSFFk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Test:\n","  def __init__(self):\n","    self.one_hot_encoded = False"],"metadata":{"id":"u8lVJS4C03ZC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ls /content/drive/MyDrive/bedford_lab/code/DRAUPNIR_ASR/data/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xEqnR28e8zMH","executionInfo":{"status":"ok","timestamp":1752084473367,"user_tz":420,"elapsed":121,"user":{"displayName":"Aayush Verma","userId":"08474324267284002295"}},"outputId":"3fc0a587-abce-41c9-e7ab-ce74035d47ee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[0m\u001b[01;34mbenchmark_randall_original_naming\u001b[0m/\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/bedford_lab/code/DRAUPNIR_ASR/data/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LSZK2qbm8sV7","executionInfo":{"status":"ok","timestamp":1752084368501,"user_tz":420,"elapsed":6,"user":{"displayName":"Aayush Verma","userId":"08474324267284002295"}},"outputId":"5657f3c5-eb5c-4628-ccda-8856240a8fe8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/bedford_lab/code/DRAUPNIR_ASR/data\n"]}]},{"cell_type":"code","source":["!export QT_QPA_PLATFORM=offscreen"],"metadata":{"id":"znzZ1zFjWj4a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!export QT_QPA_PLATFORM=offscreen && echo $QT_QPA_PLATFORM"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b-XIZSSSXdWz","executionInfo":{"status":"ok","timestamp":1752084407249,"user_tz":420,"elapsed":105,"user":{"displayName":"Aayush Verma","userId":"08474324267284002295"}},"outputId":"0a2ea21b-15c4-47e7-8d89-cc746dd7737f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["offscreen\n"]}]},{"cell_type":"code","source":["# create_draupnir_dataset(name,use_custom,script_dir,args,build=False,fasta_file=None,tree_file=None,alignment_file=None):\n","test = create_draupnir_dataset(\"benchmark_randall_original_naming\", False, \".\", args=Test(), build=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WQdlWhqVzS2e","executionInfo":{"status":"ok","timestamp":1752085274913,"user_tz":420,"elapsed":423,"user":{"displayName":"Aayush Verma","userId":"08474324267284002295"}},"outputId":"0ac7ab2e-61bd-4c56-dbfa-da84f8bfd58a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-11-2993873965.py:27: UserWarning: You have selected a pre-defined dataset, if not present, it will be downloaded. Otherwise set args.use_custom to True\n","  warnings.warn(\"You have selected a pre-defined dataset, if not present, it will be downloaded. Otherwise set args.use_custom to True\")\n","/tmp/ipython-input-6-1354994543.py:172: UserWarning: Building clades (Collapses the original tree into monophyletic clades!)\n","  warnings.warn(\"Building clades (Collapses the original tree into monophyletic clades!)\")\n"]},{"output_type":"stream","name":"stdout","text":["Analyzing alignment...\n","Reading given alignment file ...\n","Using given tree file...\n","Building patristic and cladistic matrices ...\n","Patristic matrix file already exists, not calculated\n","Ready and saved!\n","Creating not aligned dataset...\n"]}]},{"cell_type":"code","source":["pprint.pprint(test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eiDNJGcza6te","executionInfo":{"status":"ok","timestamp":1752085291668,"user_tz":420,"elapsed":44,"user":{"displayName":"Aayush Verma","userId":"08474324267284002295"}},"outputId":"6acdc8b6-88af-4828-aaaa-b0f5d9d5e35e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(BuildConfig(alignment_file='/content/drive/MyDrive/bedford_lab/code/DRAUPNIR_ASR/data/benchmark_randall_original_naming/benchmark_randall_original_naming.mafft', use_ancestral=True, n_test=0, build_graph=True, aa_prob=21, triTSNE=False, leaves_testing=False, script_dir='.', no_testing=False),\n"," SettingsConfig(one_hot_encoding=False, model_design='GP_VAE', aligned_seq=True, data_folder='/content/drive/MyDrive/bedford_lab/code/DRAUPNIR_ASR/data/benchmark_randall_original_naming', full_name=\"Randall's Coral fluorescent proteins (CFP) benchmark dataset\", tree_file='/content/drive/MyDrive/bedford_lab/code/DRAUPNIR_ASR/data/benchmark_randall_original_naming/RandallBenchmarkTree_OriginalNaming.tree'),\n"," None)\n"]}]},{"cell_type":"code","source":["def available_datasets(print_dict = False):\n","    \"\"\"Displays the available default data sets shown in the paper\"\"\"\n","    datasets = {\"simulations_blactamase_1\": \"BetaLactamase_seq\",# EvolveAGene4 Betalactamase simulation # 32 leaves\n","                \"simulations_calcitonin_1\": \"Calcitonin_seq\",# EvolveAGene4 Calcitonin simulation #50 leaves\n","                \"simulations_src_sh3_1\": \"SRC_SH3\",# EvolveAGene4 SRC SH3 domain simulation 1 #100 leaves\n","                \"simulations_sirtuins_1\": \"Sirtuin_seq\",# EvolveAGene4 Sirtuin simulation #150 leaves\n","                \"simulations_src_sh3_3\": \"SRC_SH3\",# EvolveAGene4 SRC SH3 domain simulation 2 #200 leaves\n","                \"simulations_PIGBOS_1\": \"PIGBOS_seq\",# EvolveAGene4 PIGBOS simulation #300 leaves\n","                \"simulations_insulin_2\": \"Insulin_seq\",# EvolveAGene4 Insulin simulation #400 leaves\n","                \"simulations_src_sh3_2\":\"SRC_SH3\",# EvolveAGene4 SRC SH3 domain simulation 2 #800 leaves\n","                \"simulations_jj_1\": \"jj1\",\n","                \"simulations_jj_2\": \"jj2\",\n","                \"benchmark_randall_original_naming\": None,# uses the original tree and it's original node naming\n","                \"SH3_pf00018_larger_than_30aa\":  None,# SRC kinases domain SH3 ---> Leaves and angles testing\n","                \"Coral_Faviina\":  None,  # Faviina clade from coral sequences # 35 leaves\n","                \"Coral_all\": None,# All Coral sequences (includes Faviina clade and additional sequences) #71 leaves\n","                \"PF00400\": None, # 125 real sequences\n","                \"PF00400_beta\":None,#TEST DATA\n","                \"aminopeptidase\":  None, #another real sequences example\n","                \"PF00096\": None} #another real sequences example\n","    if print_dict:\n","        pprint.pprint(datasets)\n","    datasets_full_names = {\"benchmark_randall_original_naming\":\"Randall's Coral fluorescent proteins (CFP) benchmark dataset\",  # uses the original tree and it's original node naming\n","                \"SH3_pf00018_larger_than_30aa\":\"PF00018 Pfam family of Protein Tyrosine Kinases SH3 domains\",  # SRC kinases domain SH3 ---> Leaves and angles testing\n","                \"simulations_blactamase_1\":\"32 leaves Simulation Beta-Lactamase\",  # EvolveAGene4 Betalactamase simulation\n","                \"simulations_src_sh3_1\":\"100 leaves Simulation SRC-Kinase SH3 domain\",  # EvolveAGene4 SRC SH3 domain simulation\n","                \"simulations_src_sh3_2\": \"800 leaves Simulation SRC-Kinase SH3 domain\",\n","                \"simulations_src_sh3_3\": \"200 leaves Simulation SRC-Kinase SH3 domain\",\n","                \"simulations_sirtuins_1\": \"150 leaves Simulation Sirtuin 1\",\n","                \"simulations_insulin_2\": \"400 leaves Simulation Insulin Growth Factor\",\n","                \"simulations_calcitonin_1\": \"50 leaves Simulation Calcitonin peptide\",\n","                \"simulations_PIGBOS_1\": \"300 leaves parser.add_argument('-use-cuda', type=str2bool, nargs='?',const=True, default=True, help='Use GPU')simulation PIGB Opposite Strand regulator\",\n","                \"simulations_jj_1\": \"jj1\",\n","                \"simulations_jj_2\": \"jj2\",\n","                \"Coral_Faviina\":\"Coral fluorescent proteins (CFP) Faviina clade\",  # Faviina clade from coral sequences\n","                \"Coral_all\":\"Coral fluorescent proteins (CFP) clade\",  # All Coral sequences (includes Faviina clade and additional sequences)\n","                \"PF00400\":\"WD40 125 sequences\",\n","                \"PF00400_beta\": \"WD40 125 sequences\", #TODO:Remove\n","                \"aminopeptidase\":\"Amino Peptidase\",\n","                \"PF00096\":\"PF00096 protein kinases\"}\n","    return datasets,datasets_full_names"],"metadata":{"id":"oTFlqexK4W2H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import sys #for version checker\n","import os #for restart routine\n","\n","if '3.9' in sys.version:\n","  print('You already have 3.9')\n","else:\n","  #install python 3.9 and dev utils\n","  #you may not need all the dev libraries, but I haven't tested which aren't necessary.\n","  !sudo apt-get update -y\n","  !sudo apt-get install python3.9 python3.9-dev python3.9-distutils libpython3.9-dev\n","  !sudo apt-get install python3.9-venv binfmt-support #recommended in install logs of the command above\n","\n","  #change alternatives\n","  !sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 1\n","  !sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 2\n","\n","  # install pip\n","  !curl -sS https://bootstrap.pypa.io/get-pip.py | python3.9\n","  !python3 get-pip.py --force-reinstall\n","\n","  #install colab's dependencies\n","  !python3 -m pip install setuptools ipython ipython_genutils ipykernel jupyter_console prompt_toolkit httplib2 astor\n","\n","  #minor cleanup\n","  !sudo apt autoremove\n","\n","  #link to the old google package\n","  !ln -s /usr/local/lib/python3.11/dist-packages/google /usr/local/lib/python3.9/dist-packages/google\n","  #this is just to verify if 3.9 folder was indeed created\n","  !ls /usr/local/lib/python3.9/"],"metadata":{"id":"7-muKo2ufm6n","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bf204756-7817-4c4a-e460-de9f00c01132"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["shell-init: error retrieving current directory: getcwd: cannot access parent directories: Transport endpoint is not connected\n","Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n","Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Hit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n","Hit:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n","Hit:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n","Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n","Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n","0% [Waiting for headers]"]}]},{"cell_type":"code","source":["!sudo apt-get update -y\n","!sudo apt-get install python3.9 python3.9-dev python3.9-distutils libpython3.9-dev\n","!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 1\n","!sudo update-alternatives --config python3"],"metadata":{"id":"B-FHSKzsW6tj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/bedford_lab/code/\n","!python3.9 get-pip.py"],"metadata":{"id":"Gy9y8qo-ZI9Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python --version\n","!pip --version"],"metadata":{"id":"wUtM800tadeo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/bedford_lab/code/DRAUPNIR_ASR/draupnir\n","!pip install ."],"metadata":{"id":"QbZaeKKhXeQA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install biopython; pip install pyro-ppl; pip install ete3; pip install dgl -f https://data.dgl.ai/wheels/torch-2.4/cu124/repo.html; pip install torchda"],"metadata":{"id":"Ut0oHtfTIF2N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python --version"],"metadata":{"id":"Fw3EGbK_dl-j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import sys\n","print(sys.version)"],"metadata":{"id":"j4kYniPwetd3"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7hGKSK96U1nK"},"outputs":[],"source":["# sys.path.append(\"/content/drive/MyDrive/bedford_lab/code/DRAUPNIR_ASR/draupnir/src/\")\n","# sys.path.append(\"/content/drive/MyDrive/bedford_lab/code\")\n","import draupnir"]},{"cell_type":"code","source":["!nvcc --version"],"metadata":{"id":"Y7rXDScCNjrV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(dir(draupnir))"],"metadata":{"id":"r9eyr2IKEuP3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"BUWHQ2JAms37"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["draupnir.create_draupnir_dataset(\"benchmark_randall_original_naming\", False, \"./DRAUPNIR_ASR\")"],"metadata":{"id":"fsa8-dMcDhUi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"F1PhvDEqD8VP"},"execution_count":null,"outputs":[]}]}